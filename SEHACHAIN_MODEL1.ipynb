{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEHACHAIN_MODEL1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEc8DzvAZPLgpl3l1mlnhJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaimae-kouara/SehaChain/blob/main/SEHACHAIN_MODEL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJP4zZoSeDFQ"
      },
      "source": [
        " #preparing the dataset\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl_bIXE4i_jc"
      },
      "source": [
        "data=pd.read_csv('MyDataset1.csv',delimiter=',' ,encoding= 'unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vIRYE6s5jBSh",
        "outputId": "80c48e9a-2447-4238-9ae9-24d4a0d10dda"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genre;Age ; Suivi d'un traitement ; Dure de traitement; Raison ; Susceptibilit de non respect des recommandations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0;20;1;365;3;1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1;33;1;30;1;1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1;20;1;180;3;1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1;26;1;60;0;0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1;28;1;120;3;1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Genre;Age ; Suivi d'un traitement ; Dure de traitement; Raison ; Susceptibilit de non respect des recommandations \n",
              "0                                     0;20;1;365;3;1                                                                  \n",
              "1                                      1;33;1;30;1;1                                                                  \n",
              "2                                     1;20;1;180;3;1                                                                  \n",
              "3                                      1;26;1;60;0;0                                                                  \n",
              "4                                     1;28;1;120;3;1                                                                  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q03o3asZjj-r"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4a8BGGcjJsE"
      },
      "source": [
        "dataset=np.genfromtxt('MyDataset1.csv', delimiter=';' ,skip_header=True,dtype=None , encoding= 'unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPHxlDH-l3qV",
        "outputId": "95c5c4da-7ded-41fd-e644-9284ad1d74b5"
      },
      "source": [
        "print(dataset.shape)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(407,)\n",
            "[(0, 20. , 1, 3.650e+02, 3, 1) (1, 33. , 1, 3.000e+01, 1, 1)\n",
            " (1, 20. , 1, 1.800e+02, 3, 1) (1, 26. , 1, 6.000e+01, 0, 0)\n",
            " (1, 28. , 1, 1.200e+02, 3, 1) (1, 31. , 1, 9.000e+01, 1, 1)\n",
            " (1, 33. , 1, 6.000e+01, 3, 1) (1, 30. , 0, 0.000e+00, 1, 1)\n",
            " (1, 25. , 1, 3.000e+01, 3, 1) (1, 29. , 1, 2.920e+03, 1, 1)\n",
            " (1, 27. , 1, 9.000e+01, 1, 1) (0, 34. , 1, 9.000e+01, 1, 1)\n",
            " (1, 24. , 1, 1.095e+03, 3, 1) (1, 26. , 1, 3.650e+02, 1, 1)\n",
            " (1, 22. , 0, 0.000e+00, 0, 0) (1, 23. , 1, 9.000e+01, 1, 1)\n",
            " (1, 24. , 1, 6.000e+01, 1, 1) (1, 21. , 1, 7.000e+00, 1, 1)\n",
            " (1, 23. , 1, 1.200e+02, 1, 1) (1, 21. , 1, 2.700e+02, 1, 1)\n",
            " (1, 23. , 1, 1.800e+02, 1, 1) (1, 22. , 1, 3.000e+04, 3, 1)\n",
            " (1, 23. , 0, 0.000e+00, 0, 0) (1, 21. , 1, 1.800e+02, 0, 0)\n",
            " (1, 20. , 1, 1.800e+02, 1, 1) (1, 21. , 1, 3.000e+01, 1, 1)\n",
            " (1, 22. , 1, 2.400e+02, 1, 1) (1, 22. , 1, 1.095e+03, 3, 1)\n",
            " (1, 21. , 1, 9.000e+01, 1, 1) (1, 20. , 1, 9.900e+02, 3, 1)\n",
            " (1, 22. , 1, 1.200e+02, 1, 1) (1, 20. , 1, 7.000e+00, 1, 1)\n",
            " (1, 22. , 1, 9.000e+01, 1, 1) (1, 21. , 1, 6.000e+01, 1, 1)\n",
            " (1, 21. , 1, 5.474e+03, 3, 1) (0, 23. , 0, 0.000e+00, 0, 0)\n",
            " (1, 22. , 1, 1.200e+02, 1, 1) (1, 22. , 1, 9.000e+01, 0, 0)\n",
            " (1, 23. , 0, 0.000e+00, 0, 0) (1, 25. , 1, 9.000e+01, 1, 1)\n",
            " (1, 22. , 1, 1.800e+02, 1, 1) (1, 21. , 1, 3.000e+01, 3, 1)\n",
            " (1, 21. , 1, 1.800e+02, 1, 1) (1, 18. , 0, 0.000e+00, 2, 1)\n",
            " (1, 21. , 1, 2.100e+02, 1, 1) (1, 19. , 1, 1.800e+02, 1, 1)\n",
            " (1, 20. , 1, 3.000e+01, 1, 1) (1, 24. , 1, 3.000e+01, 1, 1)\n",
            " (1, 21. , 1, 3.650e+02, 1, 1) (1, 21. , 1, 6.000e+01, 1, 1)\n",
            " (1, 22. , 1, 1.500e+01, 1, 1) (1, 22. , 0, 0.000e+00, 0, 0)\n",
            " (1, 21. , 1, 6.000e+01, 0, 0) (1, 22. , 1, 6.000e+01, 0, 0)\n",
            " (1, 26. , 1, 3.650e+02, 0, 0) (1, 29. , 1, 3.000e+04, 3, 1)\n",
            " (0, 22. , 0, 0.000e+00, 1, 1) (1, 22. , 1, 9.000e+01, 1, 1)\n",
            " (0, 22. , 1, 1.500e+01, 1, 1) (1, 26. , 1, 1.800e+02, 3, 1)\n",
            " (1, 21. , 1, 1.800e+02, 0, 0) (0, 23. , 1, 6.000e+01, 1, 1)\n",
            " (1, 21. , 1, 1.800e+02, 3, 1) (0, 22. , 1, 1.460e+03, 3, 1)\n",
            " (0, 20. , 1, 1.825e+03, 3, 1) (1, 20. , 0, 0.000e+00, 1, 1)\n",
            " (0, 22. , 1, 1.800e+02, 3, 1) (0, 23. , 1, 9.000e+01, 1, 1)\n",
            " (1, 23. , 1, 3.000e+01, 0, 0) (0, 24. , 1, 3.000e+02, 2, 1)\n",
            " (1, 21. , 1, 1.800e+02, 3, 1) (1, 21. , 1, 9.000e+01, 1, 1)\n",
            " (1, 22. , 0, 0.000e+00, 0, 0) (0, 22. , 1, 1.000e+00, 1, 1)\n",
            " (1, 28. , 1, 1.800e+02, 1, 1) (1, 33. , 1, 4.000e+02, 2, 1)\n",
            " (0, 23. , 1, 1.800e+02, 3, 1) (0, 22. , 0, 0.000e+00, 0, 0)\n",
            " (1, 22. , 1, 1.800e+02, 1, 1) (0, 23. , 1, 3.000e+01, 0, 0)\n",
            " (1, 22. , 1, 6.000e+01, 3, 1) (1, 21. , 1, 9.000e+01, 3, 1)\n",
            " (0, 25. , 0, 0.000e+00, 3, 1) (0, 22. , 0, 0.000e+00, 1, 1)\n",
            " (0, 21. , 1, 3.000e+01, 3, 1) (0, 21. , 1, 3.000e+01, 0, 0)\n",
            " (1, 22. , 1, 7.000e+00, 0, 0) (1, 23. , 1, 1.200e+02, 3, 1)\n",
            " (1, 22. , 1, 9.000e+01, 1, 1) (1, 20. , 1, 3.650e+02, 1, 1)\n",
            " (0, 23. , 1, 2.000e+00, 1, 1) (0, 29. , 1, 7.300e+02, 3, 1)\n",
            " (0, 23. , 1, 7.300e+02, 3, 1) (0, 21. , 1, 3.650e+03, 1, 1)\n",
            " (1, 29. , 1, 9.000e+01, 3, 1) (1, 20. , 1, 3.000e+04, 1, 1)\n",
            " (0, 21. , 1, 9.000e+01, 1, 1) (0, 22. , 1, 6.000e+01, 1, 1)\n",
            " (0, 21. , 1, 9.000e+01, 1, 1) (0, 22. , 1, 6.000e+01, 3, 1)\n",
            " (1, 21. , 1, 3.650e+02, 1, 1) (1, 19. , 1, 1.800e+02, 0, 0)\n",
            " (1, 18. , 1, 7.300e+02, 0, 0) (1, 18. , 1, 9.000e+01, 0, 0)\n",
            " (0, 24. , 1, 3.650e+02, 1, 1) (1, 31. , 1, 6.000e+01, 1, 1)\n",
            " (1, 20. , 1, 2.700e+02, 0, 0) (1, 21. , 1, 2.700e+02, 3, 1)\n",
            " (0, 20. , 1, 6.000e+01, 1, 1) (1, 21. , 1, 9.000e+01, 1, 1)\n",
            " (0, 22. , 0, 0.000e+00, 3, 1) (0, 23. , 1, 1.000e+01, 1, 1)\n",
            " (1, 21. , 1, 5.000e+00, 1, 1) (1, 19. , 1, 6.000e+01, 3, 1)\n",
            " (1, 20. , 1, 7.000e+00, 0, 0) (1, 22. , 1, 1.095e+03, 1, 1)\n",
            " (1, 45. , 0, 0.000e+00, 3, 1) (0, 60. , 1, 6.000e+01, 1, 1)\n",
            " (0, 34. , 1, 1.200e+02, 1, 1) (0, 21. , 1, 9.000e+01, 1, 1)\n",
            " (0, 20. , 1, 1.500e+01, 1, 1) (1, 21. , 1, 6.000e+01, 1, 1)\n",
            " (1, 20. , 1, 1.095e+03, 1, 1) (0, 20. , 1, 9.000e+01, 3, 1)\n",
            " (0, 21. , 1, 3.000e+01, 1, 1) (1, 22. , 0, 0.000e+00, 3, 1)\n",
            " (1, 20. , 0, 0.000e+00, 1, 1) (0, 22. , 1, 4.500e+01, 0, 0)\n",
            " (1, 21. , 1, 9.000e+01, 1, 1) (0, 52. , 1, 1.500e+01, 1, 1)\n",
            " (0, 23. , 1, 1.500e+01, 0, 0) (0, 24. , 1, 3.650e+02, 3, 1)\n",
            " (0, 23. , 1, 1.460e+03, 3, 1) (1, 21. , 1, 1.095e+03, 3, 1)\n",
            " (1, 23. , 0, 0.000e+00, 3, 1) (1, 33. , 1, 3.650e+03, 3, 1)\n",
            " (1, 22. , 1, 1.460e+03, 2, 1) (1, 22. , 1, 2.400e+02, 3, 1)\n",
            " (0, 56. , 1, 9.000e+01, 3, 1) (1, 21. , 1, 1.800e+02, 1, 1)\n",
            " (0, 22. , 1, 1.800e+02, 1, 1) (1, 24. , 1, 2.700e+02, 3, 1)\n",
            " (1, 21. , 1, 9.000e+01, 3, 1) (0, 23. , 0, 0.000e+00, 3, 1)\n",
            " (0, 23. , 1, 3.650e+02, 3, 1) (1, 21. , 1, 9.000e+01, 1, 1)\n",
            " (1, 23. , 1, 9.000e+01, 1, 1) (1, 25. , 1, 1.800e+02, 1, 1)\n",
            " (1, 27. , 1, 2.920e+03, 1, 1) (0, 23. , 1, 1.500e+01, 0, 0)\n",
            " (0, 24. , 1, 6.000e+01, 1, 1) (1, 20. , 1, 6.000e+01, 1, 1)\n",
            " (0, 20. , 1, 4.745e+03, 1, 1) (1, 20. , 0, 0.000e+00, 2, 1)\n",
            " (1, 20. , 1, 1.095e+03, 1, 1) (1, 21. , 1, 3.640e+02, 3, 1)\n",
            " (1, 22. , 1, 7.300e+02, 1, 1) (1, 20. , 1, 1.800e+02, 3, 1)\n",
            " (1, 22. , 1, 3.000e+01, 2, 1) (0, 23. , 1, 9.000e+01, 3, 1)\n",
            " (1, 21. , 1, 6.000e+01, 1, 1) (1, 21. , 1, 3.650e+03, 3, 1)\n",
            " (1, 22. , 1, 3.000e+01, 1, 1) (1, 21. , 1, 1.800e+02, 1, 1)\n",
            " (1, 33. , 1, 3.650e+02, 2, 1) (1, 23. , 1, 9.000e+01, 1, 1)\n",
            " (1, 45. , 1, 7.000e+00, 3, 1) (1, 31. , 1, 2.900e+01, 3, 1)\n",
            " (1, 29. , 1, 7.300e+02, 1, 1) (1, 24. , 1, 6.000e+01, 1, 1)\n",
            " (1, 21. , 1, 8.000e+01, 0, 0) (0, 20. , 0, 0.000e+00, 1, 1)\n",
            " (1, 25. , 0, 0.000e+00, 3, 1) (0, 23. , 1, 9.000e+01, 0, 0)\n",
            " (1, 22. , 1, 3.650e+02, 1, 1) (0, 23. , 1, 7.000e+00, 0, 0)\n",
            " (1, 26. , 1, 9.000e+01, 1, 1) (1, 22. , 1, 6.000e+01, 2, 1)\n",
            " (1, 22. , 1, 1.800e+02, 1, 1) (0, 40. , 1, 2.555e+03, 1, 1)\n",
            " (1, 23. , 0, 0.000e+00, 0, 0) (1, 22. , 1, 6.000e+01, 3, 1)\n",
            " (1, 23. , 1, 2.100e+02, 2, 1) (0, 61. , 1, 1.000e+01, 1, 1)\n",
            " (1, 21. , 1, 3.650e+02, 0, 0) (1, 22. , 1, 3.000e+01, 1, 1)\n",
            " (0, 33. , 1, 9.000e+01, 3, 1) (1, 20. , 1, 1.800e+02, 1, 1)\n",
            " (0, 23. , 1, 1.500e+01, 1, 1) (1, 30. , 1, 1.800e+02, 0, 0)\n",
            " (1, 24. , 1, 6.000e+01, 0, 0) (1, 21. , 1, 7.300e+02, 0, 0)\n",
            " (1, 20. , 1, 9.000e+01, 2, 1) (0, 60. , 0, 0.000e+00, 3, 1)\n",
            " (1, 22. , 1, 6.000e+01, 1, 1) (1, 20. , 0, 0.000e+00, 3, 1)\n",
            " (1, 21. , 1, 1.800e+02, 1, 1) (0, 21. , 1, 2.000e+01, 0, 0)\n",
            " (1, 21. , 1, 3.000e+01, 1, 1) (0, 21. , 1, 1.000e+01, 3, 1)\n",
            " (1, 20. , 1, 2.100e+01, 1, 1) (1, 21. , 0, 0.000e+00, 1, 1)\n",
            " (0, 21. , 1, 9.000e+01, 1, 1) (1, 21. , 1, 3.000e+01, 1, 1)\n",
            " (1, 22. , 1, 1.800e+02, 1, 1) (1, 23. , 1, 9.000e+01, 1, 1)\n",
            " (0, 52. , 1, 1.800e+02, 3, 1) (0, 20. , 1, 3.650e+02, 1, 1)\n",
            " (1, 45. , 1, 3.650e+02, 3, 1) (0, 20. , 1, 6.000e+01, 1, 1)\n",
            " (0, 58. , 0, 0.000e+00, 0, 0) (0, 22. , 0, 0.000e+00, 2, 1)\n",
            " (1, 22. , 0, 0.000e+00, 0, 0) (0, 25. , 1, 1.800e+02, 3, 1)\n",
            " (1, 19. , 1, 1.500e+01, 1, 1) (1, 22. , 1, 3.650e+02, 1, 1)\n",
            " (1, 33. , 1, 3.000e+01, 1, 1) (0, 20. , 1, 3.000e+01, 1, 1)\n",
            " (0, 67. , 1, 9.000e+01, 3, 1) (1, 25. , 1, 3.650e+02, 3, 1)\n",
            " (1, 30. , 1, 9.000e+01, 0, 0) (1, 25. , 1, 1.800e+02, 1, 1)\n",
            " (1, 57. , 1, 7.300e+02, 1, 1) (1, 25. , 1, 7.000e+00, 1, 1)\n",
            " (0, 21. , 0, 0.000e+00, 2, 1) (0, 22.5, 1, 3.000e+01, 1, 1)\n",
            " (1, 36. , 0, 1.800e+02, 0, 0) (1, 20. , 1, 6.000e+01, 2, 1)\n",
            " (1, 34. , 1, 1.800e+02, 1, 1) (0, 21. , 1, 9.000e+01, 1, 1)\n",
            " (1, 26. , 0, 0.000e+00, 0, 0) (1, 20. , 1, 3.650e+02, 2, 1)\n",
            " (1, 19. , 0, 0.000e+00, 1, 1) (1, 24. , 1, 1.500e+01, 1, 1)\n",
            " (1, 21. , 1, 1.200e+02, 0, 0) (0, 21. , 1, 9.000e+01, 3, 1)\n",
            " (0, 57. , 1, 9.000e+01, 0, 0) (1, 22. , 1, 3.000e+01, 3, 1)\n",
            " (1, 24. , 0, 0.000e+00, 3, 1) (1, 31. , 1, 6.000e+01, 1, 1)\n",
            " (1, 21. , 1, 7.300e+02, 1, 1) (1, 23. , 1, 9.000e+01, 1, 1)\n",
            " (0, 40. , 1, 6.000e+01, 0, 0) (0, 58. , 1, 1.022e+04, 3, 1)\n",
            " (1, 50. , 1, 1.000e+01, 1, 1) (0, 54. , 1, 1.800e+02, 1, 1)\n",
            " (1, 20. , 1, 6.000e+01, 1, 1) (1, 22. , 1, 3.000e+04, 0, 0)\n",
            " (0, 40. , 1, 1.800e+02, 3, 1) (0, 61. , 0, 0.000e+00, 3, 1)\n",
            " (1, 23. , 1, 1.500e+01, 1, 1) (0, 21. , 0, 0.000e+00, 1, 1)\n",
            " (1, 50. , 1, 1.400e+01, 0, 0) (0, 31. , 1, 9.000e+01, 0, 0)\n",
            " (0, 30. , 1, 1.800e+02, 2, 1) (0, 54. , 1, 3.000e+04, 3, 1)\n",
            " (1, 21. , 1, 2.700e+02, 1, 1) (0, 21. , 1, 3.000e+01, 0, 0)\n",
            " (0, 45. , 1, 3.000e+01, 2, 1) (0, 40. , 1, 1.200e+02, 0, 0)\n",
            " (1, 45. , 1, 3.650e+03, 3, 1) (1, 20. , 1, 1.825e+03, 3, 1)\n",
            " (1, 45. , 1, 7.300e+02, 3, 1) (1, 20. , 1, 6.000e+01, 1, 1)\n",
            " (0, 56. , 1, 1.500e+01, 3, 1) (0, 42. , 1, 2.100e+01, 3, 1)\n",
            " (1, 33. , 1, 1.500e+01, 3, 1) (0, 50. , 1, 9.000e+01, 1, 1)\n",
            " (1, 20. , 0, 0.000e+00, 0, 0) (1, 20. , 1, 9.000e+01, 3, 1)\n",
            " (0, 28. , 1, 1.500e+01, 1, 1) (1, 18. , 1, 3.000e+01, 1, 1)\n",
            " (0, 24. , 1, 5.000e+00, 1, 1) (0, 35. , 1, 1.800e+02, 3, 1)\n",
            " (0, 29. , 0, 0.000e+00, 0, 0) (0, 27. , 0, 0.000e+00, 0, 0)\n",
            " (1, 48. , 1, 4.015e+03, 1, 1) (0, 23. , 1, 1.500e+01, 1, 1)\n",
            " (0, 23. , 1, 1.500e+01, 1, 0) (1, 27. , 1, 1.000e+03, 3, 1)\n",
            " (0, 31. , 1, 1.800e+02, 1, 1) (1, 21. , 1, 1.300e+02, 1, 1)\n",
            " (0, 33. , 0, 0.000e+00, 0, 0) (0, 54. , 1, 7.300e+02, 3, 1)\n",
            " (1, 18. , 1, 1.000e+02, 1, 1) (1, 39. , 0, 0.000e+00, 0, 0)\n",
            " (1, 39. , 0, 0.000e+00, 0, 0) (1, 39. , 0, 0.000e+00, 0, 0)\n",
            " (1, 23. , 1, 7.300e+02, 0, 0) (0, 24. , 1, 5.000e+00, 1, 1)\n",
            " (0, 22. , 0, 0.000e+00, 0, 0) (0, 45. , 0, 0.000e+00, 0, 0)\n",
            " (0, 29. , 1, 7.000e+00, 2, 1) (0, 36. , 1, 9.000e+01, 3, 1)\n",
            " (1, 30. , 1, 1.095e+03, 3, 1) (1, 43. , 1, 3.650e+02, 3, 1)\n",
            " (0, 41. , 0, 0.000e+00, 0, 0) (1, 37. , 1, 9.000e+01, 2, 1)\n",
            " (0, 52. , 0, 0.000e+00, 0, 0) (0, 41. , 1, 8.000e+02, 3, 1)\n",
            " (1, 22. , 1, 1.500e+01, 3, 1) (1, 23. , 0, 0.000e+00, 0, 0)\n",
            " (1, 28. , 1, 2.700e+02, 0, 0) (0, 20. , 0, 0.000e+00, 0, 0)\n",
            " (1, 20. , 1, 3.000e+01, 2, 1) (0, 20. , 0, 0.000e+00, 0, 0)\n",
            " (0, 30. , 1, 3.000e+02, 1, 1) (1, 21. , 1, 2.700e+02, 0, 0)\n",
            " (1, 20. , 1, 1.500e+01, 1, 1) (1, 19. , 1, 6.000e+01, 1, 1)\n",
            " (0, 60. , 0, 0.000e+00, 0, 1) (1, 21. , 1, 1.500e+01, 0, 0)\n",
            " (1, 20. , 1, 3.000e+01, 1, 1) (0, 39. , 1, 7.000e+00, 0, 0)\n",
            " (0, 29. , 0, 0.000e+00, 0, 0) (1, 19. , 1, 3.000e+04, 3, 1)\n",
            " (0, 30. , 1, 3.650e+02, 0, 0) (0, 23. , 1, 1.095e+03, 2, 1)\n",
            " (1, 20. , 1, 1.460e+03, 2, 1) (0, 22. , 0, 0.000e+00, 2, 1)\n",
            " (1, 33. , 0, 0.000e+00, 0, 0) (0, 32. , 1, 3.000e+01, 3, 1)\n",
            " (0, 39. , 1, 1.800e+02, 0, 0) (0, 52. , 0, 0.000e+00, 1, 1)\n",
            " (0, 50. , 1, 2.555e+03, 1, 1) (1, 21. , 0, 0.000e+00, 3, 1)\n",
            " (1, 21. , 1, 9.000e+01, 1, 1) (0, 22. , 0, 0.000e+00, 1, 1)\n",
            " (1, 21. , 0, 0.000e+00, 0, 0) (1, 22. , 0, 0.000e+00, 0, 0)\n",
            " (1, 21. , 0, 0.000e+00, 0, 0) (0, 20. , 0, 0.000e+00, 1, 1)\n",
            " (1, 42. , 1, 1.800e+02, 3, 1) (0, 21. , 0, 0.000e+00, 0, 0)\n",
            " (0, 23. , 0, 0.000e+00, 0, 0) (1, 20. , 1, 1.095e+03, 0, 0)\n",
            " (1, 21. , 1, 1.500e+01, 1, 1) (0, 24. , 1, 1.800e+02, 0, 0)\n",
            " (0, 22. , 1, 9.000e+01, 1, 1) (1, 38. , 0, 0.000e+00, 0, 0)\n",
            " (0, 26. , 0, 0.000e+00, 1, 1) (1, 47. , 0, 0.000e+00, 0, 0)\n",
            " (1, 47. , 0, 0.000e+00, 0, 0) (1, 47. , 0, 0.000e+00, 0, 0)\n",
            " (1, 20. , 1, 1.000e+01, 3, 1) (1, 21. , 1, 1.800e+02, 1, 1)\n",
            " (1, 22. , 1, 7.000e+00, 1, 1) (1, 20. , 0, 0.000e+00, 3, 1)\n",
            " (1, 21. , 1, 3.650e+02, 0, 0) (0, 21. , 0, 0.000e+00, 0, 0)\n",
            " (1, 21. , 1, 6.000e+01, 0, 0) (1, 24. , 0, 0.000e+00, 0, 0)\n",
            " (0, 25. , 0, 0.000e+00, 0, 0) (1, 21. , 1, 2.100e+02, 2, 1)\n",
            " (1, 33. , 1, 9.000e+01, 0, 0) (1, 21. , 0, 0.000e+00, 0, 0)\n",
            " (0, 39. , 0, 0.000e+00, 1, 1) (0, 21. , 0, 0.000e+00, 2, 1)\n",
            " (0, 54. , 1, 9.000e+01, 1, 1) (0, 47. , 1, 9.000e+01, 1, 1)\n",
            " (0, 45. , 1, 3.000e+01, 1, 1) (0, 43. , 1, 3.000e+04, 1, 1)\n",
            " (0, 45. , 0, 0.000e+00, 1, 1) (1, 27. , 1, 9.000e+01, 2, 1)\n",
            " (0, 45. , 1, 9.000e+01, 2, 1) (1, 20. , 0, 0.000e+00, 1, 1)\n",
            " (0, 40. , 0, 0.000e+00, 2, 0) (1, 20. , 0, 0.000e+00, 1, 1)\n",
            " (1, 23. , 0, 0.000e+00, 1, 1) (1, 20. , 1, 3.000e+04, 0, 0)\n",
            " (0, 23. , 0, 0.000e+00, 3, 1) (0, 26. , 1, 1.500e+01, 1, 0)\n",
            " (0, 37. , 1, 9.000e+01, 0, 0) (0, 48. , 0, 0.000e+00, 0, 0)\n",
            " (0, 30. , 0, 0.000e+00, 1, 1) (0, 24. , 1, 6.000e+01, 3, 1)\n",
            " (0, 24. , 1, 6.000e+01, 3, 1) (0, 34. , 1, 1.500e+01, 3, 1)\n",
            " (1, 23. , 1, 9.000e+01, 3, 1) (1, 20. , 1, 1.000e+03, 3, 1)\n",
            " (1, 21. , 1, 3.650e+02, 0, 0) (0, 23. , 0, 0.000e+00, 0, 0)\n",
            " (1, 39. , 0, 0.000e+00, 0, 0) (0, 29. , 1, 1.000e+01, 1, 1)\n",
            " (0, 28. , 0, 0.000e+00, 3, 1) (1, 24. , 0, 0.000e+00, 0, 0)\n",
            " (1, 22. , 0, 0.000e+00, 0, 0) (1, 20. , 0, 0.000e+00, 0, 0)\n",
            " (1, 26. , 0, 0.000e+00, 0, 0) (1, 22. , 1, 7.000e+00, 1, 1)\n",
            " (1, 31. , 0, 0.000e+00, 0, 0) (1, 20. , 0, 0.000e+00, 0, 0)\n",
            " (1, 30. , 0, 0.000e+00, 3, 1) (1, 27. , 0, 0.000e+00, 1, 1)\n",
            " (1, 25. , 1, 7.300e+02, 3, 1) (1, 25. , 0, 0.000e+00, 1, 1)\n",
            " (1, 26. , 0, 0.000e+00, 0, 0) (1, 23. , 0, 0.000e+00, 2, 1)\n",
            " (0, 33. , 0, 0.000e+00, 3, 1) (0, 27. , 1, 1.800e+02, 1, 1)\n",
            " (0, 35. , 0, 0.000e+00, 0, 0) (1, 20. , 1, 2.555e+03, 2, 1)\n",
            " (1, 20. , 1, 2.555e+03, 2, 1) (0, 26. , 0, 0.000e+00, 2, 1)\n",
            " (0, 21. , 1, 1.800e+02, 1, 1) (0, 30. , 1, 7.000e+00, 1, 1)\n",
            " (0, 31. , 1, 7.000e+00, 3, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdSPCtoYmuU5"
      },
      "source": [
        "np.set_printoptions(formatter={'float':'{:0.1f}'.format})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDrej-uWmC87",
        "outputId": "98c66d2d-008d-4333-a2a9-d2d0a388a2a2"
      },
      "source": [
        "print(dataset)\n",
        "#print(dataset[77])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 20.0, 1, 365.0, 3, 1) (1, 33.0, 1, 30.0, 1, 1)\n",
            " (1, 20.0, 1, 180.0, 3, 1) (1, 26.0, 1, 60.0, 0, 0)\n",
            " (1, 28.0, 1, 120.0, 3, 1) (1, 31.0, 1, 90.0, 1, 1)\n",
            " (1, 33.0, 1, 60.0, 3, 1) (1, 30.0, 0, 0.0, 1, 1) (1, 25.0, 1, 30.0, 3, 1)\n",
            " (1, 29.0, 1, 2920.0, 1, 1) (1, 27.0, 1, 90.0, 1, 1)\n",
            " (0, 34.0, 1, 90.0, 1, 1) (1, 24.0, 1, 1095.0, 3, 1)\n",
            " (1, 26.0, 1, 365.0, 1, 1) (1, 22.0, 0, 0.0, 0, 0)\n",
            " (1, 23.0, 1, 90.0, 1, 1) (1, 24.0, 1, 60.0, 1, 1) (1, 21.0, 1, 7.0, 1, 1)\n",
            " (1, 23.0, 1, 120.0, 1, 1) (1, 21.0, 1, 270.0, 1, 1)\n",
            " (1, 23.0, 1, 180.0, 1, 1) (1, 22.0, 1, 30000.0, 3, 1)\n",
            " (1, 23.0, 0, 0.0, 0, 0) (1, 21.0, 1, 180.0, 0, 0)\n",
            " (1, 20.0, 1, 180.0, 1, 1) (1, 21.0, 1, 30.0, 1, 1)\n",
            " (1, 22.0, 1, 240.0, 1, 1) (1, 22.0, 1, 1095.0, 3, 1)\n",
            " (1, 21.0, 1, 90.0, 1, 1) (1, 20.0, 1, 990.0, 3, 1)\n",
            " (1, 22.0, 1, 120.0, 1, 1) (1, 20.0, 1, 7.0, 1, 1)\n",
            " (1, 22.0, 1, 90.0, 1, 1) (1, 21.0, 1, 60.0, 1, 1)\n",
            " (1, 21.0, 1, 5474.0, 3, 1) (0, 23.0, 0, 0.0, 0, 0)\n",
            " (1, 22.0, 1, 120.0, 1, 1) (1, 22.0, 1, 90.0, 0, 0)\n",
            " (1, 23.0, 0, 0.0, 0, 0) (1, 25.0, 1, 90.0, 1, 1)\n",
            " (1, 22.0, 1, 180.0, 1, 1) (1, 21.0, 1, 30.0, 3, 1)\n",
            " (1, 21.0, 1, 180.0, 1, 1) (1, 18.0, 0, 0.0, 2, 1)\n",
            " (1, 21.0, 1, 210.0, 1, 1) (1, 19.0, 1, 180.0, 1, 1)\n",
            " (1, 20.0, 1, 30.0, 1, 1) (1, 24.0, 1, 30.0, 1, 1)\n",
            " (1, 21.0, 1, 365.0, 1, 1) (1, 21.0, 1, 60.0, 1, 1)\n",
            " (1, 22.0, 1, 15.0, 1, 1) (1, 22.0, 0, 0.0, 0, 0) (1, 21.0, 1, 60.0, 0, 0)\n",
            " (1, 22.0, 1, 60.0, 0, 0) (1, 26.0, 1, 365.0, 0, 0)\n",
            " (1, 29.0, 1, 30000.0, 3, 1) (0, 22.0, 0, 0.0, 1, 1)\n",
            " (1, 22.0, 1, 90.0, 1, 1) (0, 22.0, 1, 15.0, 1, 1)\n",
            " (1, 26.0, 1, 180.0, 3, 1) (1, 21.0, 1, 180.0, 0, 0)\n",
            " (0, 23.0, 1, 60.0, 1, 1) (1, 21.0, 1, 180.0, 3, 1)\n",
            " (0, 22.0, 1, 1460.0, 3, 1) (0, 20.0, 1, 1825.0, 3, 1)\n",
            " (1, 20.0, 0, 0.0, 1, 1) (0, 22.0, 1, 180.0, 3, 1)\n",
            " (0, 23.0, 1, 90.0, 1, 1) (1, 23.0, 1, 30.0, 0, 0)\n",
            " (0, 24.0, 1, 300.0, 2, 1) (1, 21.0, 1, 180.0, 3, 1)\n",
            " (1, 21.0, 1, 90.0, 1, 1) (1, 22.0, 0, 0.0, 0, 0) (0, 22.0, 1, 1.0, 1, 1)\n",
            " (1, 28.0, 1, 180.0, 1, 1) (1, 33.0, 1, 400.0, 2, 1)\n",
            " (0, 23.0, 1, 180.0, 3, 1) (0, 22.0, 0, 0.0, 0, 0)\n",
            " (1, 22.0, 1, 180.0, 1, 1) (0, 23.0, 1, 30.0, 0, 0)\n",
            " (1, 22.0, 1, 60.0, 3, 1) (1, 21.0, 1, 90.0, 3, 1) (0, 25.0, 0, 0.0, 3, 1)\n",
            " (0, 22.0, 0, 0.0, 1, 1) (0, 21.0, 1, 30.0, 3, 1) (0, 21.0, 1, 30.0, 0, 0)\n",
            " (1, 22.0, 1, 7.0, 0, 0) (1, 23.0, 1, 120.0, 3, 1)\n",
            " (1, 22.0, 1, 90.0, 1, 1) (1, 20.0, 1, 365.0, 1, 1)\n",
            " (0, 23.0, 1, 2.0, 1, 1) (0, 29.0, 1, 730.0, 3, 1)\n",
            " (0, 23.0, 1, 730.0, 3, 1) (0, 21.0, 1, 3650.0, 1, 1)\n",
            " (1, 29.0, 1, 90.0, 3, 1) (1, 20.0, 1, 30000.0, 1, 1)\n",
            " (0, 21.0, 1, 90.0, 1, 1) (0, 22.0, 1, 60.0, 1, 1)\n",
            " (0, 21.0, 1, 90.0, 1, 1) (0, 22.0, 1, 60.0, 3, 1)\n",
            " (1, 21.0, 1, 365.0, 1, 1) (1, 19.0, 1, 180.0, 0, 0)\n",
            " (1, 18.0, 1, 730.0, 0, 0) (1, 18.0, 1, 90.0, 0, 0)\n",
            " (0, 24.0, 1, 365.0, 1, 1) (1, 31.0, 1, 60.0, 1, 1)\n",
            " (1, 20.0, 1, 270.0, 0, 0) (1, 21.0, 1, 270.0, 3, 1)\n",
            " (0, 20.0, 1, 60.0, 1, 1) (1, 21.0, 1, 90.0, 1, 1) (0, 22.0, 0, 0.0, 3, 1)\n",
            " (0, 23.0, 1, 10.0, 1, 1) (1, 21.0, 1, 5.0, 1, 1) (1, 19.0, 1, 60.0, 3, 1)\n",
            " (1, 20.0, 1, 7.0, 0, 0) (1, 22.0, 1, 1095.0, 1, 1)\n",
            " (1, 45.0, 0, 0.0, 3, 1) (0, 60.0, 1, 60.0, 1, 1)\n",
            " (0, 34.0, 1, 120.0, 1, 1) (0, 21.0, 1, 90.0, 1, 1)\n",
            " (0, 20.0, 1, 15.0, 1, 1) (1, 21.0, 1, 60.0, 1, 1)\n",
            " (1, 20.0, 1, 1095.0, 1, 1) (0, 20.0, 1, 90.0, 3, 1)\n",
            " (0, 21.0, 1, 30.0, 1, 1) (1, 22.0, 0, 0.0, 3, 1) (1, 20.0, 0, 0.0, 1, 1)\n",
            " (0, 22.0, 1, 45.0, 0, 0) (1, 21.0, 1, 90.0, 1, 1)\n",
            " (0, 52.0, 1, 15.0, 1, 1) (0, 23.0, 1, 15.0, 0, 0)\n",
            " (0, 24.0, 1, 365.0, 3, 1) (0, 23.0, 1, 1460.0, 3, 1)\n",
            " (1, 21.0, 1, 1095.0, 3, 1) (1, 23.0, 0, 0.0, 3, 1)\n",
            " (1, 33.0, 1, 3650.0, 3, 1) (1, 22.0, 1, 1460.0, 2, 1)\n",
            " (1, 22.0, 1, 240.0, 3, 1) (0, 56.0, 1, 90.0, 3, 1)\n",
            " (1, 21.0, 1, 180.0, 1, 1) (0, 22.0, 1, 180.0, 1, 1)\n",
            " (1, 24.0, 1, 270.0, 3, 1) (1, 21.0, 1, 90.0, 3, 1)\n",
            " (0, 23.0, 0, 0.0, 3, 1) (0, 23.0, 1, 365.0, 3, 1)\n",
            " (1, 21.0, 1, 90.0, 1, 1) (1, 23.0, 1, 90.0, 1, 1)\n",
            " (1, 25.0, 1, 180.0, 1, 1) (1, 27.0, 1, 2920.0, 1, 1)\n",
            " (0, 23.0, 1, 15.0, 0, 0) (0, 24.0, 1, 60.0, 1, 1)\n",
            " (1, 20.0, 1, 60.0, 1, 1) (0, 20.0, 1, 4745.0, 1, 1)\n",
            " (1, 20.0, 0, 0.0, 2, 1) (1, 20.0, 1, 1095.0, 1, 1)\n",
            " (1, 21.0, 1, 364.0, 3, 1) (1, 22.0, 1, 730.0, 1, 1)\n",
            " (1, 20.0, 1, 180.0, 3, 1) (1, 22.0, 1, 30.0, 2, 1)\n",
            " (0, 23.0, 1, 90.0, 3, 1) (1, 21.0, 1, 60.0, 1, 1)\n",
            " (1, 21.0, 1, 3650.0, 3, 1) (1, 22.0, 1, 30.0, 1, 1)\n",
            " (1, 21.0, 1, 180.0, 1, 1) (1, 33.0, 1, 365.0, 2, 1)\n",
            " (1, 23.0, 1, 90.0, 1, 1) (1, 45.0, 1, 7.0, 3, 1) (1, 31.0, 1, 29.0, 3, 1)\n",
            " (1, 29.0, 1, 730.0, 1, 1) (1, 24.0, 1, 60.0, 1, 1)\n",
            " (1, 21.0, 1, 80.0, 0, 0) (0, 20.0, 0, 0.0, 1, 1) (1, 25.0, 0, 0.0, 3, 1)\n",
            " (0, 23.0, 1, 90.0, 0, 0) (1, 22.0, 1, 365.0, 1, 1)\n",
            " (0, 23.0, 1, 7.0, 0, 0) (1, 26.0, 1, 90.0, 1, 1) (1, 22.0, 1, 60.0, 2, 1)\n",
            " (1, 22.0, 1, 180.0, 1, 1) (0, 40.0, 1, 2555.0, 1, 1)\n",
            " (1, 23.0, 0, 0.0, 0, 0) (1, 22.0, 1, 60.0, 3, 1)\n",
            " (1, 23.0, 1, 210.0, 2, 1) (0, 61.0, 1, 10.0, 1, 1)\n",
            " (1, 21.0, 1, 365.0, 0, 0) (1, 22.0, 1, 30.0, 1, 1)\n",
            " (0, 33.0, 1, 90.0, 3, 1) (1, 20.0, 1, 180.0, 1, 1)\n",
            " (0, 23.0, 1, 15.0, 1, 1) (1, 30.0, 1, 180.0, 0, 0)\n",
            " (1, 24.0, 1, 60.0, 0, 0) (1, 21.0, 1, 730.0, 0, 0)\n",
            " (1, 20.0, 1, 90.0, 2, 1) (0, 60.0, 0, 0.0, 3, 1) (1, 22.0, 1, 60.0, 1, 1)\n",
            " (1, 20.0, 0, 0.0, 3, 1) (1, 21.0, 1, 180.0, 1, 1)\n",
            " (0, 21.0, 1, 20.0, 0, 0) (1, 21.0, 1, 30.0, 1, 1)\n",
            " (0, 21.0, 1, 10.0, 3, 1) (1, 20.0, 1, 21.0, 1, 1) (1, 21.0, 0, 0.0, 1, 1)\n",
            " (0, 21.0, 1, 90.0, 1, 1) (1, 21.0, 1, 30.0, 1, 1)\n",
            " (1, 22.0, 1, 180.0, 1, 1) (1, 23.0, 1, 90.0, 1, 1)\n",
            " (0, 52.0, 1, 180.0, 3, 1) (0, 20.0, 1, 365.0, 1, 1)\n",
            " (1, 45.0, 1, 365.0, 3, 1) (0, 20.0, 1, 60.0, 1, 1)\n",
            " (0, 58.0, 0, 0.0, 0, 0) (0, 22.0, 0, 0.0, 2, 1) (1, 22.0, 0, 0.0, 0, 0)\n",
            " (0, 25.0, 1, 180.0, 3, 1) (1, 19.0, 1, 15.0, 1, 1)\n",
            " (1, 22.0, 1, 365.0, 1, 1) (1, 33.0, 1, 30.0, 1, 1)\n",
            " (0, 20.0, 1, 30.0, 1, 1) (0, 67.0, 1, 90.0, 3, 1)\n",
            " (1, 25.0, 1, 365.0, 3, 1) (1, 30.0, 1, 90.0, 0, 0)\n",
            " (1, 25.0, 1, 180.0, 1, 1) (1, 57.0, 1, 730.0, 1, 1)\n",
            " (1, 25.0, 1, 7.0, 1, 1) (0, 21.0, 0, 0.0, 2, 1) (0, 22.5, 1, 30.0, 1, 1)\n",
            " (1, 36.0, 0, 180.0, 0, 0) (1, 20.0, 1, 60.0, 2, 1)\n",
            " (1, 34.0, 1, 180.0, 1, 1) (0, 21.0, 1, 90.0, 1, 1)\n",
            " (1, 26.0, 0, 0.0, 0, 0) (1, 20.0, 1, 365.0, 2, 1) (1, 19.0, 0, 0.0, 1, 1)\n",
            " (1, 24.0, 1, 15.0, 1, 1) (1, 21.0, 1, 120.0, 0, 0)\n",
            " (0, 21.0, 1, 90.0, 3, 1) (0, 57.0, 1, 90.0, 0, 0)\n",
            " (1, 22.0, 1, 30.0, 3, 1) (1, 24.0, 0, 0.0, 3, 1) (1, 31.0, 1, 60.0, 1, 1)\n",
            " (1, 21.0, 1, 730.0, 1, 1) (1, 23.0, 1, 90.0, 1, 1)\n",
            " (0, 40.0, 1, 60.0, 0, 0) (0, 58.0, 1, 10220.0, 3, 1)\n",
            " (1, 50.0, 1, 10.0, 1, 1) (0, 54.0, 1, 180.0, 1, 1)\n",
            " (1, 20.0, 1, 60.0, 1, 1) (1, 22.0, 1, 30000.0, 0, 0)\n",
            " (0, 40.0, 1, 180.0, 3, 1) (0, 61.0, 0, 0.0, 3, 1)\n",
            " (1, 23.0, 1, 15.0, 1, 1) (0, 21.0, 0, 0.0, 1, 1) (1, 50.0, 1, 14.0, 0, 0)\n",
            " (0, 31.0, 1, 90.0, 0, 0) (0, 30.0, 1, 180.0, 2, 1)\n",
            " (0, 54.0, 1, 30000.0, 3, 1) (1, 21.0, 1, 270.0, 1, 1)\n",
            " (0, 21.0, 1, 30.0, 0, 0) (0, 45.0, 1, 30.0, 2, 1)\n",
            " (0, 40.0, 1, 120.0, 0, 0) (1, 45.0, 1, 3650.0, 3, 1)\n",
            " (1, 20.0, 1, 1825.0, 3, 1) (1, 45.0, 1, 730.0, 3, 1)\n",
            " (1, 20.0, 1, 60.0, 1, 1) (0, 56.0, 1, 15.0, 3, 1)\n",
            " (0, 42.0, 1, 21.0, 3, 1) (1, 33.0, 1, 15.0, 3, 1)\n",
            " (0, 50.0, 1, 90.0, 1, 1) (1, 20.0, 0, 0.0, 0, 0) (1, 20.0, 1, 90.0, 3, 1)\n",
            " (0, 28.0, 1, 15.0, 1, 1) (1, 18.0, 1, 30.0, 1, 1) (0, 24.0, 1, 5.0, 1, 1)\n",
            " (0, 35.0, 1, 180.0, 3, 1) (0, 29.0, 0, 0.0, 0, 0) (0, 27.0, 0, 0.0, 0, 0)\n",
            " (1, 48.0, 1, 4015.0, 1, 1) (0, 23.0, 1, 15.0, 1, 1)\n",
            " (0, 23.0, 1, 15.0, 1, 0) (1, 27.0, 1, 1000.0, 3, 1)\n",
            " (0, 31.0, 1, 180.0, 1, 1) (1, 21.0, 1, 130.0, 1, 1)\n",
            " (0, 33.0, 0, 0.0, 0, 0) (0, 54.0, 1, 730.0, 3, 1)\n",
            " (1, 18.0, 1, 100.0, 1, 1) (1, 39.0, 0, 0.0, 0, 0) (1, 39.0, 0, 0.0, 0, 0)\n",
            " (1, 39.0, 0, 0.0, 0, 0) (1, 23.0, 1, 730.0, 0, 0) (0, 24.0, 1, 5.0, 1, 1)\n",
            " (0, 22.0, 0, 0.0, 0, 0) (0, 45.0, 0, 0.0, 0, 0) (0, 29.0, 1, 7.0, 2, 1)\n",
            " (0, 36.0, 1, 90.0, 3, 1) (1, 30.0, 1, 1095.0, 3, 1)\n",
            " (1, 43.0, 1, 365.0, 3, 1) (0, 41.0, 0, 0.0, 0, 0)\n",
            " (1, 37.0, 1, 90.0, 2, 1) (0, 52.0, 0, 0.0, 0, 0)\n",
            " (0, 41.0, 1, 800.0, 3, 1) (1, 22.0, 1, 15.0, 3, 1)\n",
            " (1, 23.0, 0, 0.0, 0, 0) (1, 28.0, 1, 270.0, 0, 0) (0, 20.0, 0, 0.0, 0, 0)\n",
            " (1, 20.0, 1, 30.0, 2, 1) (0, 20.0, 0, 0.0, 0, 0)\n",
            " (0, 30.0, 1, 300.0, 1, 1) (1, 21.0, 1, 270.0, 0, 0)\n",
            " (1, 20.0, 1, 15.0, 1, 1) (1, 19.0, 1, 60.0, 1, 1) (0, 60.0, 0, 0.0, 0, 1)\n",
            " (1, 21.0, 1, 15.0, 0, 0) (1, 20.0, 1, 30.0, 1, 1) (0, 39.0, 1, 7.0, 0, 0)\n",
            " (0, 29.0, 0, 0.0, 0, 0) (1, 19.0, 1, 30000.0, 3, 1)\n",
            " (0, 30.0, 1, 365.0, 0, 0) (0, 23.0, 1, 1095.0, 2, 1)\n",
            " (1, 20.0, 1, 1460.0, 2, 1) (0, 22.0, 0, 0.0, 2, 1)\n",
            " (1, 33.0, 0, 0.0, 0, 0) (0, 32.0, 1, 30.0, 3, 1)\n",
            " (0, 39.0, 1, 180.0, 0, 0) (0, 52.0, 0, 0.0, 1, 1)\n",
            " (0, 50.0, 1, 2555.0, 1, 1) (1, 21.0, 0, 0.0, 3, 1)\n",
            " (1, 21.0, 1, 90.0, 1, 1) (0, 22.0, 0, 0.0, 1, 1) (1, 21.0, 0, 0.0, 0, 0)\n",
            " (1, 22.0, 0, 0.0, 0, 0) (1, 21.0, 0, 0.0, 0, 0) (0, 20.0, 0, 0.0, 1, 1)\n",
            " (1, 42.0, 1, 180.0, 3, 1) (0, 21.0, 0, 0.0, 0, 0) (0, 23.0, 0, 0.0, 0, 0)\n",
            " (1, 20.0, 1, 1095.0, 0, 0) (1, 21.0, 1, 15.0, 1, 1)\n",
            " (0, 24.0, 1, 180.0, 0, 0) (0, 22.0, 1, 90.0, 1, 1)\n",
            " (1, 38.0, 0, 0.0, 0, 0) (0, 26.0, 0, 0.0, 1, 1) (1, 47.0, 0, 0.0, 0, 0)\n",
            " (1, 47.0, 0, 0.0, 0, 0) (1, 47.0, 0, 0.0, 0, 0) (1, 20.0, 1, 10.0, 3, 1)\n",
            " (1, 21.0, 1, 180.0, 1, 1) (1, 22.0, 1, 7.0, 1, 1) (1, 20.0, 0, 0.0, 3, 1)\n",
            " (1, 21.0, 1, 365.0, 0, 0) (0, 21.0, 0, 0.0, 0, 0)\n",
            " (1, 21.0, 1, 60.0, 0, 0) (1, 24.0, 0, 0.0, 0, 0) (0, 25.0, 0, 0.0, 0, 0)\n",
            " (1, 21.0, 1, 210.0, 2, 1) (1, 33.0, 1, 90.0, 0, 0)\n",
            " (1, 21.0, 0, 0.0, 0, 0) (0, 39.0, 0, 0.0, 1, 1) (0, 21.0, 0, 0.0, 2, 1)\n",
            " (0, 54.0, 1, 90.0, 1, 1) (0, 47.0, 1, 90.0, 1, 1)\n",
            " (0, 45.0, 1, 30.0, 1, 1) (0, 43.0, 1, 30000.0, 1, 1)\n",
            " (0, 45.0, 0, 0.0, 1, 1) (1, 27.0, 1, 90.0, 2, 1) (0, 45.0, 1, 90.0, 2, 1)\n",
            " (1, 20.0, 0, 0.0, 1, 1) (0, 40.0, 0, 0.0, 2, 0) (1, 20.0, 0, 0.0, 1, 1)\n",
            " (1, 23.0, 0, 0.0, 1, 1) (1, 20.0, 1, 30000.0, 0, 0)\n",
            " (0, 23.0, 0, 0.0, 3, 1) (0, 26.0, 1, 15.0, 1, 0) (0, 37.0, 1, 90.0, 0, 0)\n",
            " (0, 48.0, 0, 0.0, 0, 0) (0, 30.0, 0, 0.0, 1, 1) (0, 24.0, 1, 60.0, 3, 1)\n",
            " (0, 24.0, 1, 60.0, 3, 1) (0, 34.0, 1, 15.0, 3, 1)\n",
            " (1, 23.0, 1, 90.0, 3, 1) (1, 20.0, 1, 1000.0, 3, 1)\n",
            " (1, 21.0, 1, 365.0, 0, 0) (0, 23.0, 0, 0.0, 0, 0) (1, 39.0, 0, 0.0, 0, 0)\n",
            " (0, 29.0, 1, 10.0, 1, 1) (0, 28.0, 0, 0.0, 3, 1) (1, 24.0, 0, 0.0, 0, 0)\n",
            " (1, 22.0, 0, 0.0, 0, 0) (1, 20.0, 0, 0.0, 0, 0) (1, 26.0, 0, 0.0, 0, 0)\n",
            " (1, 22.0, 1, 7.0, 1, 1) (1, 31.0, 0, 0.0, 0, 0) (1, 20.0, 0, 0.0, 0, 0)\n",
            " (1, 30.0, 0, 0.0, 3, 1) (1, 27.0, 0, 0.0, 1, 1) (1, 25.0, 1, 730.0, 3, 1)\n",
            " (1, 25.0, 0, 0.0, 1, 1) (1, 26.0, 0, 0.0, 0, 0) (1, 23.0, 0, 0.0, 2, 1)\n",
            " (0, 33.0, 0, 0.0, 3, 1) (0, 27.0, 1, 180.0, 1, 1) (0, 35.0, 0, 0.0, 0, 0)\n",
            " (1, 20.0, 1, 2555.0, 2, 1) (1, 20.0, 1, 2555.0, 2, 1)\n",
            " (0, 26.0, 0, 0.0, 2, 1) (0, 21.0, 1, 180.0, 1, 1) (0, 30.0, 1, 7.0, 1, 1)\n",
            " (0, 31.0, 1, 7.0, 3, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOCoZmopl1hm"
      },
      "source": [
        "df=np.empty([407,6], dtype=float)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gbseIOdoT7w",
        "outputId": "9f23721f-8d3c-44b1-db12-49d5511d4dee"
      },
      "source": [
        "for i in range(len(df)):\n",
        "  for j in range(len(df[i])):\n",
        "   df[i][j]=dataset[i][j].astype(float)\n",
        "  print(df[i])\n",
        "  \n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0 20.0 1.0 365.0 3.0 1.0]\n",
            "[1.0 33.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 180.0 3.0 1.0]\n",
            "[1.0 26.0 1.0 60.0 0.0 0.0]\n",
            "[1.0 28.0 1.0 120.0 3.0 1.0]\n",
            "[1.0 31.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 33.0 1.0 60.0 3.0 1.0]\n",
            "[1.0 30.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 25.0 1.0 30.0 3.0 1.0]\n",
            "[1.0 29.0 1.0 2920.0 1.0 1.0]\n",
            "[1.0 27.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 34.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 24.0 1.0 1095.0 3.0 1.0]\n",
            "[1.0 26.0 1.0 365.0 1.0 1.0]\n",
            "[1.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 23.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 24.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 7.0 1.0 1.0]\n",
            "[1.0 23.0 1.0 120.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 270.0 1.0 1.0]\n",
            "[1.0 23.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 30000.0 3.0 1.0]\n",
            "[1.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 180.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 240.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 1095.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 990.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 120.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 7.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 5474.0 3.0 1.0]\n",
            "[0.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 120.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 90.0 0.0 0.0]\n",
            "[1.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 25.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 30.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 18.0 0.0 0.0 2.0 1.0]\n",
            "[1.0 21.0 1.0 210.0 1.0 1.0]\n",
            "[1.0 19.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 24.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 365.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 60.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 60.0 0.0 0.0]\n",
            "[1.0 26.0 1.0 365.0 0.0 0.0]\n",
            "[1.0 29.0 1.0 30000.0 3.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 22.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 26.0 1.0 180.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 0.0 0.0]\n",
            "[0.0 23.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 22.0 1.0 1460.0 3.0 1.0]\n",
            "[0.0 20.0 1.0 1825.0 3.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 22.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 23.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 23.0 1.0 30.0 0.0 0.0]\n",
            "[0.0 24.0 1.0 300.0 2.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 22.0 1.0 1.0 1.0 1.0]\n",
            "[1.0 28.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 33.0 1.0 400.0 2.0 1.0]\n",
            "[0.0 23.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 30.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 60.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 3.0 1.0]\n",
            "[0.0 25.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 30.0 3.0 1.0]\n",
            "[0.0 21.0 1.0 30.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 7.0 0.0 0.0]\n",
            "[1.0 23.0 1.0 120.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 365.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 2.0 1.0 1.0]\n",
            "[0.0 29.0 1.0 730.0 3.0 1.0]\n",
            "[0.0 23.0 1.0 730.0 3.0 1.0]\n",
            "[0.0 21.0 1.0 3650.0 1.0 1.0]\n",
            "[1.0 29.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 30000.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 22.0 1.0 60.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 22.0 1.0 60.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 365.0 1.0 1.0]\n",
            "[1.0 19.0 1.0 180.0 0.0 0.0]\n",
            "[1.0 18.0 1.0 730.0 0.0 0.0]\n",
            "[1.0 18.0 1.0 90.0 0.0 0.0]\n",
            "[0.0 24.0 1.0 365.0 1.0 1.0]\n",
            "[1.0 31.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 270.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 270.0 3.0 1.0]\n",
            "[0.0 20.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 23.0 1.0 10.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 5.0 1.0 1.0]\n",
            "[1.0 19.0 1.0 60.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 7.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 1095.0 1.0 1.0]\n",
            "[1.0 45.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 60.0 1.0 60.0 1.0 1.0]\n",
            "[0.0 34.0 1.0 120.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 20.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 1095.0 1.0 1.0]\n",
            "[0.0 20.0 1.0 90.0 3.0 1.0]\n",
            "[0.0 21.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 22.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 22.0 1.0 45.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 52.0 1.0 15.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 15.0 0.0 0.0]\n",
            "[0.0 24.0 1.0 365.0 3.0 1.0]\n",
            "[0.0 23.0 1.0 1460.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 1095.0 3.0 1.0]\n",
            "[1.0 23.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 33.0 1.0 3650.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 1460.0 2.0 1.0]\n",
            "[1.0 22.0 1.0 240.0 3.0 1.0]\n",
            "[0.0 56.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 22.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 24.0 1.0 270.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 3.0 1.0]\n",
            "[0.0 23.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 23.0 1.0 365.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 23.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 25.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 27.0 1.0 2920.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 15.0 0.0 0.0]\n",
            "[0.0 24.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 60.0 1.0 1.0]\n",
            "[0.0 20.0 1.0 4745.0 1.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 2.0 1.0]\n",
            "[1.0 20.0 1.0 1095.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 364.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 730.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 180.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 30.0 2.0 1.0]\n",
            "[0.0 23.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 3650.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 33.0 1.0 365.0 2.0 1.0]\n",
            "[1.0 23.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 45.0 1.0 7.0 3.0 1.0]\n",
            "[1.0 31.0 1.0 29.0 3.0 1.0]\n",
            "[1.0 29.0 1.0 730.0 1.0 1.0]\n",
            "[1.0 24.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 80.0 0.0 0.0]\n",
            "[0.0 20.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 25.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 23.0 1.0 90.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 365.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 7.0 0.0 0.0]\n",
            "[1.0 26.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 60.0 2.0 1.0]\n",
            "[1.0 22.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 40.0 1.0 2555.0 1.0 1.0]\n",
            "[1.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 60.0 3.0 1.0]\n",
            "[1.0 23.0 1.0 210.0 2.0 1.0]\n",
            "[0.0 61.0 1.0 10.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 365.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 33.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 30.0 1.0 180.0 0.0 0.0]\n",
            "[1.0 24.0 1.0 60.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 730.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 90.0 2.0 1.0]\n",
            "[0.0 60.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 20.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 10.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 21.0 1.0 1.0]\n",
            "[1.0 21.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 30.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 23.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 52.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 20.0 1.0 365.0 1.0 1.0]\n",
            "[1.0 45.0 1.0 365.0 3.0 1.0]\n",
            "[0.0 20.0 1.0 60.0 1.0 1.0]\n",
            "[0.0 58.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 22.0 0.0 0.0 2.0 1.0]\n",
            "[1.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 25.0 1.0 180.0 3.0 1.0]\n",
            "[1.0 19.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 365.0 1.0 1.0]\n",
            "[1.0 33.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 20.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 67.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 25.0 1.0 365.0 3.0 1.0]\n",
            "[1.0 30.0 1.0 90.0 0.0 0.0]\n",
            "[1.0 25.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 57.0 1.0 730.0 1.0 1.0]\n",
            "[1.0 25.0 1.0 7.0 1.0 1.0]\n",
            "[0.0 21.0 0.0 0.0 2.0 1.0]\n",
            "[0.0 22.5 1.0 30.0 1.0 1.0]\n",
            "[1.0 36.0 0.0 180.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 60.0 2.0 1.0]\n",
            "[1.0 34.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 26.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 365.0 2.0 1.0]\n",
            "[1.0 19.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 24.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 120.0 0.0 0.0]\n",
            "[0.0 21.0 1.0 90.0 3.0 1.0]\n",
            "[0.0 57.0 1.0 90.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 30.0 3.0 1.0]\n",
            "[1.0 24.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 31.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 730.0 1.0 1.0]\n",
            "[1.0 23.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 40.0 1.0 60.0 0.0 0.0]\n",
            "[0.0 58.0 1.0 10220.0 3.0 1.0]\n",
            "[1.0 50.0 1.0 10.0 1.0 1.0]\n",
            "[0.0 54.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 60.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 30000.0 0.0 0.0]\n",
            "[0.0 40.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 61.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 23.0 1.0 15.0 1.0 1.0]\n",
            "[0.0 21.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 50.0 1.0 14.0 0.0 0.0]\n",
            "[0.0 31.0 1.0 90.0 0.0 0.0]\n",
            "[0.0 30.0 1.0 180.0 2.0 1.0]\n",
            "[0.0 54.0 1.0 30000.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 270.0 1.0 1.0]\n",
            "[0.0 21.0 1.0 30.0 0.0 0.0]\n",
            "[0.0 45.0 1.0 30.0 2.0 1.0]\n",
            "[0.0 40.0 1.0 120.0 0.0 0.0]\n",
            "[1.0 45.0 1.0 3650.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 1825.0 3.0 1.0]\n",
            "[1.0 45.0 1.0 730.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 60.0 1.0 1.0]\n",
            "[0.0 56.0 1.0 15.0 3.0 1.0]\n",
            "[0.0 42.0 1.0 21.0 3.0 1.0]\n",
            "[1.0 33.0 1.0 15.0 3.0 1.0]\n",
            "[0.0 50.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 90.0 3.0 1.0]\n",
            "[0.0 28.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 18.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 24.0 1.0 5.0 1.0 1.0]\n",
            "[0.0 35.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 29.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 27.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 48.0 1.0 4015.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 15.0 1.0 1.0]\n",
            "[0.0 23.0 1.0 15.0 1.0 0.0]\n",
            "[1.0 27.0 1.0 1000.0 3.0 1.0]\n",
            "[0.0 31.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 130.0 1.0 1.0]\n",
            "[0.0 33.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 54.0 1.0 730.0 3.0 1.0]\n",
            "[1.0 18.0 1.0 100.0 1.0 1.0]\n",
            "[1.0 39.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 39.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 39.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 23.0 1.0 730.0 0.0 0.0]\n",
            "[0.0 24.0 1.0 5.0 1.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 45.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 29.0 1.0 7.0 2.0 1.0]\n",
            "[0.0 36.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 30.0 1.0 1095.0 3.0 1.0]\n",
            "[1.0 43.0 1.0 365.0 3.0 1.0]\n",
            "[0.0 41.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 37.0 1.0 90.0 2.0 1.0]\n",
            "[0.0 52.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 41.0 1.0 800.0 3.0 1.0]\n",
            "[1.0 22.0 1.0 15.0 3.0 1.0]\n",
            "[1.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 28.0 1.0 270.0 0.0 0.0]\n",
            "[0.0 20.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 30.0 2.0 1.0]\n",
            "[0.0 20.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 30.0 1.0 300.0 1.0 1.0]\n",
            "[1.0 21.0 1.0 270.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 15.0 1.0 1.0]\n",
            "[1.0 19.0 1.0 60.0 1.0 1.0]\n",
            "[0.0 60.0 0.0 0.0 0.0 1.0]\n",
            "[1.0 21.0 1.0 15.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 39.0 1.0 7.0 0.0 0.0]\n",
            "[0.0 29.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 19.0 1.0 30000.0 3.0 1.0]\n",
            "[0.0 30.0 1.0 365.0 0.0 0.0]\n",
            "[0.0 23.0 1.0 1095.0 2.0 1.0]\n",
            "[1.0 20.0 1.0 1460.0 2.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 2.0 1.0]\n",
            "[1.0 33.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 32.0 1.0 30.0 3.0 1.0]\n",
            "[0.0 39.0 1.0 180.0 0.0 0.0]\n",
            "[0.0 52.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 50.0 1.0 2555.0 1.0 1.0]\n",
            "[1.0 21.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 22.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 21.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 21.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 20.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 42.0 1.0 180.0 3.0 1.0]\n",
            "[0.0 21.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 1095.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 15.0 1.0 1.0]\n",
            "[0.0 24.0 1.0 180.0 0.0 0.0]\n",
            "[0.0 22.0 1.0 90.0 1.0 1.0]\n",
            "[1.0 38.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 26.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 47.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 47.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 47.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 10.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 180.0 1.0 1.0]\n",
            "[1.0 22.0 1.0 7.0 1.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 365.0 0.0 0.0]\n",
            "[0.0 21.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 60.0 0.0 0.0]\n",
            "[1.0 24.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 25.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 21.0 1.0 210.0 2.0 1.0]\n",
            "[1.0 33.0 1.0 90.0 0.0 0.0]\n",
            "[1.0 21.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 39.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 21.0 0.0 0.0 2.0 1.0]\n",
            "[0.0 54.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 47.0 1.0 90.0 1.0 1.0]\n",
            "[0.0 45.0 1.0 30.0 1.0 1.0]\n",
            "[0.0 43.0 1.0 30000.0 1.0 1.0]\n",
            "[0.0 45.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 27.0 1.0 90.0 2.0 1.0]\n",
            "[0.0 45.0 1.0 90.0 2.0 1.0]\n",
            "[1.0 20.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 40.0 0.0 0.0 2.0 0.0]\n",
            "[1.0 20.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 23.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 20.0 1.0 30000.0 0.0 0.0]\n",
            "[0.0 23.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 26.0 1.0 15.0 1.0 0.0]\n",
            "[0.0 37.0 1.0 90.0 0.0 0.0]\n",
            "[0.0 48.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 30.0 0.0 0.0 1.0 1.0]\n",
            "[0.0 24.0 1.0 60.0 3.0 1.0]\n",
            "[0.0 24.0 1.0 60.0 3.0 1.0]\n",
            "[0.0 34.0 1.0 15.0 3.0 1.0]\n",
            "[1.0 23.0 1.0 90.0 3.0 1.0]\n",
            "[1.0 20.0 1.0 1000.0 3.0 1.0]\n",
            "[1.0 21.0 1.0 365.0 0.0 0.0]\n",
            "[0.0 23.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 39.0 0.0 0.0 0.0 0.0]\n",
            "[0.0 29.0 1.0 10.0 1.0 1.0]\n",
            "[0.0 28.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 24.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 22.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 26.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 22.0 1.0 7.0 1.0 1.0]\n",
            "[1.0 31.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 30.0 0.0 0.0 3.0 1.0]\n",
            "[1.0 27.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 25.0 1.0 730.0 3.0 1.0]\n",
            "[1.0 25.0 0.0 0.0 1.0 1.0]\n",
            "[1.0 26.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 23.0 0.0 0.0 2.0 1.0]\n",
            "[0.0 33.0 0.0 0.0 3.0 1.0]\n",
            "[0.0 27.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 35.0 0.0 0.0 0.0 0.0]\n",
            "[1.0 20.0 1.0 2555.0 2.0 1.0]\n",
            "[1.0 20.0 1.0 2555.0 2.0 1.0]\n",
            "[0.0 26.0 0.0 0.0 2.0 1.0]\n",
            "[0.0 21.0 1.0 180.0 1.0 1.0]\n",
            "[0.0 30.0 1.0 7.0 1.0 1.0]\n",
            "[0.0 31.0 1.0 7.0 3.0 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiaBtYz6yisb",
        "outputId": "822508ef-7655-4868-9a65-9e2e1493e0dc"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 20.0 1.0 365.0 3.0 1.0]\n",
            " [1.0 33.0 1.0 30.0 1.0 1.0]\n",
            " [1.0 20.0 1.0 180.0 3.0 1.0]\n",
            " ...\n",
            " [0.0 21.0 1.0 180.0 1.0 1.0]\n",
            " [0.0 30.0 1.0 7.0 1.0 1.0]\n",
            " [0.0 31.0 1.0 7.0 3.0 1.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHEUn3MKs0Ub",
        "outputId": "506f9163-ef2d-4ccc-ee4d-70bac3621d9a"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(407, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du-WmjZZ1v7S"
      },
      "source": [
        "#shuffling the data\n",
        "import random\n",
        "np.random.shuffle(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYTFiJ8xKbzu",
        "outputId": "dfafa6b0-9762-47af-abac-4f70a8a54ce1"
      },
      "source": [
        "print(df)\n",
        "print(df[77])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 22.0 1.0 15.0 3.0 1.0]\n",
            " [1.0 21.0 1.0 210.0 2.0 1.0]\n",
            " [1.0 21.0 1.0 60.0 0.0 0.0]\n",
            " ...\n",
            " [1.0 20.0 1.0 2555.0 2.0 1.0]\n",
            " [1.0 23.0 1.0 90.0 1.0 1.0]\n",
            " [1.0 47.0 0.0 0.0 0.0 0.0]]\n",
            "[1.0 20.0 0.0 0.0 1.0 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9-zGJ5eKtzS",
        "outputId": "018e1715-041c-4ee7-9f70-b847be44d8a9"
      },
      "source": [
        "#spliting the dataset into training and validation dataset\n",
        "index=int(0.2*len(df[:,0]))\n",
        "print(index)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trHDq2lPN6Z3",
        "outputId": "b011733b-9075-4705-ea8f-d751055ea238"
      },
      "source": [
        "XValidation=df[:index, :-2]\n",
        "YValidation=df[:index, -1]\n",
        "XTrain=df[index: , :-2]\n",
        "YTrain=df[index: ,-1]\n",
        "print(XValidation[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 22.0 1.0 15.0]\n",
            " [1.0 21.0 1.0 210.0]\n",
            " [1.0 21.0 1.0 60.0]\n",
            " [0.0 25.0 0.0 0.0]\n",
            " [1.0 27.0 0.0 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gBHQjJ0N9NB"
      },
      "source": [
        "#Data normalization\n",
        "mean=XTrain.mean(axis=0)####Moyenne\n",
        "XTrain -=mean\n",
        "std=XTrain.std(axis=0)##\"Ecart-type\n",
        "XTrain /=std\n",
        "XValidation -=mean\n",
        "XValidation /=std\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVypgrIfmwL2",
        "outputId": "b7de6a9d-574a-4fab-b82b-52cff2137a72"
      },
      "source": [
        "print(mean)\n",
        "print(std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6 26.9 0.7 839.5]\n",
            "[0.5 9.9 0.4 4086.7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "FAKGJ9fxndhz",
        "outputId": "dd7a3cc6-13c9-4efb-a4e6-7a34e68ef725"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "plt.hist([XTrain[: , 1]])\n",
        "plt.ylabel('Age')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "dIgtPz07spRn",
        "outputId": "ef638156-b0b2-44aa-cae9-9b4849e75fff"
      },
      "source": [
        "plt.hist([XTrain[: , 3]])\n",
        "plt.ylabel('Durée de traitement')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUhklEQVR4nO3df7BfdX3n8efLgIpIRUtkskkw6GZxsaOhvSIujkOhtIBUZHaXgY4WLds4W2yh22kFdi10pmxxt5W6O5Zp+NFGiyD+YKGWpaWUllqrmNBU5FebQhiSRRIqP8sIAu/943vu4ZJ8772Hm3y/597k+Zj5zvecz/n1TiD3dc/nnPM5qSokSQJ4Rd8FSJLmD0NBktQyFCRJLUNBktQyFCRJrb36LmBnHHDAAbVixYq+y5CkBWX9+vWPVNXiYcsWdCisWLGCdevW9V2GJC0oSR6YbpndR5KklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKklqEgSWoZCpKk1oJ+onlnrDjnT3o79qaL3tfbsSVpJp4pSJJahoIkqWUoSJJahoIkqTWyUEjy6iS3Jfn7JHcm+Y2m/eAk30yyMckXkryyaX9VM7+xWb5iVLVJkoYb5ZnCM8DRVfUOYBVwXJIjgE8CF1fVvwYeBc5o1j8DeLRpv7hZT5I0RiMLhRp4qpndu/kUcDTwpaZ9LfCBZvqkZp5m+TFJMqr6JEk7Guk1hSSLkmwAtgI3Af8EPFZVzzWrbAaWNtNLgQcBmuWPAz88ZJ+rk6xLsm7btm2jLF+S9jgjDYWqer6qVgHLgMOBt+6Cfa6pqomqmli8eOgrRiVJczSWu4+q6jHgFuDdwP5JJp+kXgZsaaa3AMsBmuWvA/55HPVJkgZGeffR4iT7N9P7AMcCdzMIh//QrHY6cF0zfX0zT7P8L6qqRlWfJGlHoxz7aAmwNskiBuFzTVV9NcldwNVJfhP4O+DyZv3Lgc8l2Qh8Dzh1hLVJkoYYWShU1beBw4a038fg+sL27d8H/uOo6pEkzc4nmiVJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQaWSgkWZ7kliR3JbkzyVlN+wVJtiTZ0HxOmLLNuUk2Jrk3yU+NqjZJ0nB7jXDfzwG/UlW3J9kPWJ/kpmbZxVX121NXTnIocCrwNuBfAX+e5N9U1fMjrFGSNMXIzhSq6qGqur2ZfhK4G1g6wyYnAVdX1TNVdT+wETh8VPVJknY0lmsKSVYAhwHfbJo+luTbSa5I8vqmbSnw4JTNNjMkRJKsTrIuybpt27aNsGpJ2vOMPBSSvBb4MnB2VT0BXAK8BVgFPAT8zsvZX1WtqaqJqppYvHjxLq9XkvZkIw2FJHszCIQrq+orAFX1cFU9X1UvAJfyYhfRFmD5lM2XNW2SpDEZ5d1HAS4H7q6qT01pXzJltZOB7zTT1wOnJnlVkoOBlcBto6pPkrSjUd59dCTwIeCOJBuatvOA05KsAgrYBHwUoKruTHINcBeDO5fO9M4jSRqvkYVCVX0NyJBFN8ywzYXAhaOqSZI0M59oliS1DAVJUmvWUEjyuS5tkqSFr8uZwtumziRZBPzYaMqRJPVp2lBoBqd7Enh7kieaz5PAVuC6sVUoSRqbaUOhqn6rqvYD/mdV/VDz2a+qfriqzh1jjZKkMZn1ltSqOjfJUuBNU9evqltHWZgkafxmDYUkFzEY0vouYPJhsgIMBUnazXR5eO1k4JCqembUxUiS+tXl7qP7gL1HXYgkqX9dzhSeBjYkuRlozxaq6pdGVpUkqRddQuH65iNJ2s11uftobZJ9gIOq6t4x1CRJ6kmXYS5+GtgA3NjMr0rimYMk7Ya6XGi+gMHb0R4DqKoNwJtHWJMkqSddQuEHVfX4dm0vjKIYSVK/ulxovjPJzwCLkqwEfgn4+mjLkiT1ocuZwi8yGCn1GeAq4Ang7FEWJUnqR5e7j54G/mvzkSTtxrqMfTQBnAes4KUD4r19dGVJkvrQ5ZrClcCvAnfgBWZJ2q11CYVtVeVzCZK0B+gSCucnuQzYfuyjr4ysKklSL7qEwkeAtzIYKXWy+6gAQ0GSdjNdQuGdVXXIy91xkuXAZ4EDGYTImqr6dJI3AF9gcOF6E3BKVT2aJMCngRMYjMz64aq6/eUeV5I0d12eU/h6kkPnsO/ngF+pqkOBI4Azm/2cA9xcVSsZdEmd06x/PLCy+awGLpnDMSVJO6HLmcIRDN6ncD+DawoBarZbUqvqIeChZvrJJHcDS4GTgKOa1dYCfwl8vGn/bFUV8I0k+ydZ0uxHkjQGXULhuJ09SJIVwGHAN4EDp/yg/y6D7iUYBMaDUzbb3LS9JBSSrGZwJsFBBx20s6VJkqaYtfuoqh4AlgNHN9NPd9luUpLXAl8Gzq6qJ7bbdzG43tBZVa2pqomqmli8ePHL2VSSNIsu71M4n0H3zrlN097AH3XZeZK9GQTClVNuYX04yZJm+RJga9O+hUH4TFrWtEmSxqTLb/wnA+8H/gWgqv4fsN9sGzV3E10O3F1Vn5qy6Hrg9Gb6dOC6Ke0/m4EjgMe9niBJ49XlmsKzVVVJCiDJvh33fSTwIeCOJBuatvOAi4BrkpwBPACc0iy7gcHtqBsZdFF9pONxJEm7SJdQuCbJ7wP7J/l54OeAy2bbqKq+xuBOpWGOGbJ+AWd2qEeSNCJdhs7+7STHMniPwiHAr1fVTSOvTJI0dl2Gzv5kVX0cuGlImyRpN9LlQvOxQ9qO39WFSJL6N+2ZQpL/DPwC8OYk356yaD/gb0ZdmCRp/GbqPvo88H+B3+LF8YkAnqyq7420KklSL2YKhaqqTUl2uCMoyRsMBkna/cx2pnAisJ7BUBRTby8t4M0jrEuS1INpQ6GqTmy+Dx5fOZKkPnV5eI0kr2fwnoNXT7ZV1a2jKkqS1I8uzyn8J+AsBgPUbWDwfoW/BY4ebWmSpHHr8pzCWcA7gQeq6scZvBfhsZFWJUnqRZdQ+H5VfR8gyauq6h4Gw11IknYzXa4pbE6yP/B/gJuSPMpgdFNJ0m6my4B4JzeTFyS5BXgdcONIq5Ik9WLGUEiyCLizqt4KUFV/NZaqJEm9mPGaQlU9D9yb5KAx1SNJ6lGXawqvB+5MchvNKzkBqur9I6tKktSLLqHwiZFXIUmaF7qEwgnbv1AnyScBry9I0m7Gl+xIklq+ZEeS1PIlO5Kk1kxDZz8OPA6cNr5yJEl96nJNQZK0hxhZKCS5IsnWJN+Z0nZBki1JNjSfE6YsOzfJxiT3JvmpUdUlSZpep1BI8qYkP9FM75Nkvw6b/SFw3JD2i6tqVfO5odnnocCpwNuabX6vGWJDkjRGs4ZCkp8HvgT8ftO0jMGIqTNq3szW9YL0ScDVVfVMVd0PbAQO77itJGkX6XKmcCZwJPAEQFX9I/DGnTjmx5J8u+leen3TthR4cMo6m5s2SdIYdQmFZ6rq2cmZJHsBNcfjXQK8BVgFPAT8zsvdQZLVSdYlWbdt27Y5liFJGqZLKPxVkvOAfZIcC3wR+OO5HKyqHq6q56vqBeBSXuwi2gIsn7LqsqZt2D7WVNVEVU0sXrx4LmVIkqbRJRTOAbYBdwAfBW4A/ttcDpZkyZTZk4HJO5OuB05N8qokBwMrgdvmcgxJ0tx1efPa5G/1l76cHSe5CjgKOCDJZuB84Kgkqxh0P21iEDJU1Z1JrgHuAp4Dzmze5SBJGqOZxj66gxmuHVTV22facVUNexL68hnWvxC4cKZ9SpJGa6YzhROb7zOb78813x9k7heaJUnz2ExjHz0AkOTYqjpsyqKPJ7mdlw6SJ0naDXS50JwkR06Z+Xcdt5MkLTBd3rx2BnBFktc1848BPze6kiRJfely99F64B2TodAMqS1J2g11OVMADANJ2hN4bUCS1DIUJEmtLkNnvybJJ5Jc2syvTHLibNtJkhaeLmcKfwA8A7y7md8C/ObIKpIk9aZLKLylqv4H8AOAqnoayEirkiT1oksoPJtkH5qhLZK8hcGZgyRpN9PlltTzgRuB5UmuZPAWtg+PsihJUj+6PLx2UzPW0REMuo3OqqpHRl6ZJGnsutx9FOB44Meq6qvAa5IcPstmkqQFaGgoJHlPkkXN7O8xuPNo8v0ITwKfGUNtkqQxm+5M4QXgkmb6XVV1JvB9gKp6FHjlGGqTJI3Z0GsKVfX1JE83sz9ozhom7z5azCA0JEm7mWmvKVTVhmbyfwHXAm9MciHwNeC/j6E2SdKYzXj3UZJXAPcDvwYcw+Duow9U1d1jqE2SNGYzhkJVvZDkM83rOO8ZU02SpJ50eaL55iT/vrk1VZK0G+sSCh8Fvgg8k+SJJE8meWLEdUmSetDlieb9xlGIJKl/s4ZCkvcOa6+qW2fZ7grgRGBrVf1I0/YG4AvACmATcEpVPdp0TX0aOAF4GvhwVd3e/Y8hSdoVunQf/eqUzyeAPwYu6LDdHwLHbdd2DnBzVa0Ebm7mYTCMxsrms5oXH5yTJI1Rl+6jn546n2Q58Lsdtrs1yYrtmk8Cjmqm1wJ/CXy8af9sVRXwjST7J1lSVQ/NdhxJ0q4zl3c0bwb+7RyPd+CUH/TfBQ5sppcCD253jKVzPIYkaY66XFP43zRDXDAIkVXATvf3V1UlqdnX3KGe1Qy6mDjooIN2tgxJ0hRdXrKzbsr0c8BVVfU3czzew5PdQkmWAFub9i3A8inrLWvadlBVa4A1ABMTEy87VCRJ0+tyTWFtMwgeVbVtJ493PXA6cFHzfd2U9o8luRp4F/C41xMkafymvaaQgQuSPALcC/xDkm1Jfr3LjpNcBfwtcEiSzUnOYBAGxyb5R+AnmnmAG4D7gI3ApcAvzPlPJEmas5nOFH6ZwfuY31lV9wMkeTNwSZJfrqqLZ9pxVZ02zaJjhqxbwJndSpYkjcpMdx99CDhtMhAAquo+4IPAz466MEnS+M0UCntX1SPbNzbXFfYeXUmSpL7MFArPznGZJGmBmumawjumGQ01wKtHVI8kqUfThkJVLRpnIZKk/s1lmAtJ0m7KUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktQwFSVLLUJAktfbq46BJNgFPAs8Dz1XVRJI3AF8AVgCbgFOq6tE+6pOkPVWfZwo/XlWrqmqimT8HuLmqVgI3N/OSpDGaT91HJwFrm+m1wAd6rEWS9kh9hUIBf5ZkfZLVTduBVfVQM/1d4MBhGyZZnWRdknXbtm0bR62StMfo5ZoC8J6q2pLkjcBNSe6ZurCqKkkN27Cq1gBrACYmJoauI0mam17OFKpqS/O9FbgWOBx4OMkSgOZ7ax+1SdKebOyhkGTfJPtNTgM/CXwHuB44vVntdOC6cdcmSXu6PrqPDgSuTTJ5/M9X1Y1JvgVck+QM4AHglB5qk6Q92thDoaruA94xpP2fgWPGXY8k6UXz6ZZUSVLPDAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS19uq7gD3RinP+pJfjbrrofb0cV9pd9fVvGUb373nenSkkOS7JvUk2Jjmn73okaU8yr0IhySLgM8DxwKHAaUkO7bcqSdpzzLfuo8OBjVV1H0CSq4GTgLt6rUo7zS4zaWGYb6GwFHhwyvxm4F1TV0iyGljdzD6V5N4R1nMA8MgI978rdK4xnxxxJTPr5e/yZf6ZF8J/b1gYdS6EGmFh1Dm0xp389/ym6RbMt1CYVVWtAdaM41hJ1lXVxDiONVcLoUZYGHUuhBphYdS5EGqEhVHnuGucV9cUgC3A8inzy5o2SdIYzLdQ+BawMsnBSV4JnApc33NNkrTHmFfdR1X1XJKPAX8KLAKuqKo7eyxpLN1UO2kh1AgLo86FUCMsjDoXQo2wMOoca42pqnEeT5I0j8237iNJUo8MBUlSy1AYYiEMtZHkiiRbk3yn71qmk2R5kluS3JXkziRn9V3TMEleneS2JH/f1Pkbfdc0nSSLkvxdkq/2Xct0kmxKckeSDUnW9V3PMEn2T/KlJPckuTvJu/uuaXtJDmn+Dic/TyQ5e+TH9ZrCSzVDbfwDcCyDh+e+BZxWVfPqqeok7wWeAj5bVT/Sdz3DJFkCLKmq25PsB6wHPjAP/y4D7FtVTyXZG/gacFZVfaPn0naQ5L8AE8APVdWJfdczTJJNwERVzduHwpKsBf66qi5r7nR8TVU91ndd02l+Lm0B3lVVD4zyWJ4p7KgdaqOqngUmh9qYV6rqVuB7fdcxk6p6qKpub6afBO5m8NT6vFIDTzWzezefeffbUpJlwPuAy/quZSFL8jrgvcDlAFX17HwOhMYxwD+NOhDAUBhm2FAb8+4H2UKTZAVwGPDNfisZrumW2QBsBW6qqvlY5+8Cvwa80Hchsyjgz5Ksb4almW8OBrYBf9B0xV2WZN++i5rFqcBV4ziQoaCRS/Ja4MvA2VX1RN/1DFNVz1fVKgZP0R+eZF51ySU5EdhaVev7rqWD91TVjzIY7fjMpqtzPtkL+FHgkqo6DPgXYF5eOwRourfeD3xxHMczFHbkUBu7UNNH/2Xgyqr6St/1zKbpRrgFOK7vWrZzJPD+pr/+auDoJH/Ub0nDVdWW5nsrcC2DLtn5ZDOwecrZ4JcYhMR8dTxwe1U9PI6DGQo7cqiNXaS5gHs5cHdVfarveqaTZHGS/ZvpfRjcZHBPv1W9VFWdW1XLqmoFg/8n/6KqPthzWTtIsm9zUwFNl8xPAvPqDrmq+i7wYJJDmqZjmN/D85/GmLqOYJ4NczEfzMOhNoZKchVwFHBAks3A+VV1eb9V7eBI4EPAHU1/PcB5VXVDjzUNswRY29zh8Qrgmqqat7d8znMHAtcOfh9gL+DzVXVjvyUN9YvAlc0vfvcBH+m5nqGaYD0W+OjYjuktqZKkSXYfSZJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJa/x9J6er0ZIIhIgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCQRwC2s3E0",
        "outputId": "658f50a5-ec78-4bac-9a33-cdb0236cafaf"
      },
      "source": [
        "print(XTrain.shape)\n",
        "print(XValidation.shape)\n",
        "print(YTrain.shape)\n",
        "print(YValidation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(326, 4)\n",
            "(81, 4)\n",
            "(326,)\n",
            "(81,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESIIgb2dxpRr",
        "outputId": "a1ad3a9a-6101-4be8-8eca-496864ffea36"
      },
      "source": [
        "#Creating the model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model=Sequential()\n",
        "model.add(Dense(4,input_dim=len(XTrain[0, : ]),activation='relu'))\n",
        "model.add(Dense(3,activation='relu'))\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 15        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 39\n",
            "Trainable params: 39\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNQ3BGvbYOP_"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gesTDp2Mlm3r"
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "callback1=ModelCheckpoint(filepath='MyBestModel.hdf5',monitor='val_loss',save_best_only=True,save_weights=True)\n",
        "callback2=EarlyStopping(monitor='val_loss',mode='min',patience=20,verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBfV3fwaiPMl",
        "outputId": "5132bad4-d556-43c2-d048-be52565be9e5"
      },
      "source": [
        "history=model.fit(XTrain , YTrain , validation_data=(XValidation,YValidation),epochs=250 , batch_size=10, callbacks=[callback1,callback2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "33/33 [==============================] - 1s 6ms/step - loss: 0.6936 - accuracy: 0.6380 - val_loss: 0.6861 - val_accuracy: 0.7407\n",
            "Epoch 2/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.7147 - val_loss: 0.6796 - val_accuracy: 0.7531\n",
            "Epoch 3/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6781 - accuracy: 0.7239 - val_loss: 0.6725 - val_accuracy: 0.7531\n",
            "Epoch 4/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.7331 - val_loss: 0.6655 - val_accuracy: 0.7531\n",
            "Epoch 5/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.7362 - val_loss: 0.6561 - val_accuracy: 0.7531\n",
            "Epoch 6/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.7362 - val_loss: 0.6454 - val_accuracy: 0.7531\n",
            "Epoch 7/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.7362 - val_loss: 0.6355 - val_accuracy: 0.7531\n",
            "Epoch 8/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.7362 - val_loss: 0.6271 - val_accuracy: 0.7531\n",
            "Epoch 9/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.7362 - val_loss: 0.6139 - val_accuracy: 0.7531\n",
            "Epoch 10/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.6065 - accuracy: 0.7362 - val_loss: 0.5954 - val_accuracy: 0.7531\n",
            "Epoch 11/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.7362 - val_loss: 0.5794 - val_accuracy: 0.7531\n",
            "Epoch 12/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5735 - accuracy: 0.7362 - val_loss: 0.5664 - val_accuracy: 0.7531\n",
            "Epoch 13/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.7362 - val_loss: 0.5574 - val_accuracy: 0.7531\n",
            "Epoch 14/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7362 - val_loss: 0.5499 - val_accuracy: 0.7531\n",
            "Epoch 15/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5469 - accuracy: 0.7362 - val_loss: 0.5452 - val_accuracy: 0.7531\n",
            "Epoch 16/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.7362 - val_loss: 0.5406 - val_accuracy: 0.7531\n",
            "Epoch 17/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7362 - val_loss: 0.5379 - val_accuracy: 0.7531\n",
            "Epoch 18/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7362 - val_loss: 0.5354 - val_accuracy: 0.7531\n",
            "Epoch 19/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7362 - val_loss: 0.5337 - val_accuracy: 0.7531\n",
            "Epoch 20/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7362 - val_loss: 0.5320 - val_accuracy: 0.7531\n",
            "Epoch 21/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7362 - val_loss: 0.5307 - val_accuracy: 0.7531\n",
            "Epoch 22/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7362 - val_loss: 0.5300 - val_accuracy: 0.7531\n",
            "Epoch 23/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7362 - val_loss: 0.5289 - val_accuracy: 0.7531\n",
            "Epoch 24/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5312 - accuracy: 0.7362 - val_loss: 0.5281 - val_accuracy: 0.7531\n",
            "Epoch 25/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7362 - val_loss: 0.5272 - val_accuracy: 0.7531\n",
            "Epoch 26/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7362 - val_loss: 0.5268 - val_accuracy: 0.7531\n",
            "Epoch 27/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7362 - val_loss: 0.5259 - val_accuracy: 0.7531\n",
            "Epoch 28/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7362 - val_loss: 0.5252 - val_accuracy: 0.7531\n",
            "Epoch 29/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7362 - val_loss: 0.5246 - val_accuracy: 0.7531\n",
            "Epoch 30/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7362 - val_loss: 0.5241 - val_accuracy: 0.7531\n",
            "Epoch 31/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7362 - val_loss: 0.5235 - val_accuracy: 0.7531\n",
            "Epoch 32/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7362 - val_loss: 0.5230 - val_accuracy: 0.7531\n",
            "Epoch 33/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7362 - val_loss: 0.5226 - val_accuracy: 0.7531\n",
            "Epoch 34/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7362 - val_loss: 0.5224 - val_accuracy: 0.7531\n",
            "Epoch 35/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7362 - val_loss: 0.5218 - val_accuracy: 0.7531\n",
            "Epoch 36/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7362 - val_loss: 0.5214 - val_accuracy: 0.7531\n",
            "Epoch 37/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7362 - val_loss: 0.5211 - val_accuracy: 0.7531\n",
            "Epoch 38/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7362 - val_loss: 0.5208 - val_accuracy: 0.7531\n",
            "Epoch 39/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7362 - val_loss: 0.5203 - val_accuracy: 0.7531\n",
            "Epoch 40/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7362 - val_loss: 0.5201 - val_accuracy: 0.7531\n",
            "Epoch 41/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7362 - val_loss: 0.5198 - val_accuracy: 0.7531\n",
            "Epoch 42/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5267 - accuracy: 0.7362 - val_loss: 0.5194 - val_accuracy: 0.7531\n",
            "Epoch 43/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7362 - val_loss: 0.5191 - val_accuracy: 0.7531\n",
            "Epoch 44/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7362 - val_loss: 0.5186 - val_accuracy: 0.7531\n",
            "Epoch 45/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7362 - val_loss: 0.5186 - val_accuracy: 0.7531\n",
            "Epoch 46/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7362 - val_loss: 0.5184 - val_accuracy: 0.7531\n",
            "Epoch 47/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7362 - val_loss: 0.5179 - val_accuracy: 0.7531\n",
            "Epoch 48/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7362 - val_loss: 0.5178 - val_accuracy: 0.7531\n",
            "Epoch 49/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7362 - val_loss: 0.5174 - val_accuracy: 0.7531\n",
            "Epoch 50/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7362 - val_loss: 0.5171 - val_accuracy: 0.7531\n",
            "Epoch 51/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7362 - val_loss: 0.5169 - val_accuracy: 0.7531\n",
            "Epoch 52/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7362 - val_loss: 0.5167 - val_accuracy: 0.7531\n",
            "Epoch 53/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.7362 - val_loss: 0.5166 - val_accuracy: 0.7531\n",
            "Epoch 54/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7362 - val_loss: 0.5163 - val_accuracy: 0.7531\n",
            "Epoch 55/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7362 - val_loss: 0.5160 - val_accuracy: 0.7531\n",
            "Epoch 56/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7362 - val_loss: 0.5161 - val_accuracy: 0.7531\n",
            "Epoch 57/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7362 - val_loss: 0.5160 - val_accuracy: 0.7531\n",
            "Epoch 58/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7362 - val_loss: 0.5156 - val_accuracy: 0.7531\n",
            "Epoch 59/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7362 - val_loss: 0.5153 - val_accuracy: 0.7531\n",
            "Epoch 60/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5245 - accuracy: 0.7362 - val_loss: 0.5153 - val_accuracy: 0.7531\n",
            "Epoch 61/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5245 - accuracy: 0.7362 - val_loss: 0.5151 - val_accuracy: 0.7531\n",
            "Epoch 62/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7362 - val_loss: 0.5147 - val_accuracy: 0.7531\n",
            "Epoch 63/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.7362 - val_loss: 0.5145 - val_accuracy: 0.7531\n",
            "Epoch 64/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7362 - val_loss: 0.5145 - val_accuracy: 0.7531\n",
            "Epoch 65/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7362 - val_loss: 0.5144 - val_accuracy: 0.7531\n",
            "Epoch 66/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7362 - val_loss: 0.5139 - val_accuracy: 0.7531\n",
            "Epoch 67/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7362 - val_loss: 0.5135 - val_accuracy: 0.7531\n",
            "Epoch 68/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7362 - val_loss: 0.5135 - val_accuracy: 0.7531\n",
            "Epoch 69/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7362 - val_loss: 0.5134 - val_accuracy: 0.7531\n",
            "Epoch 70/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5237 - accuracy: 0.7362 - val_loss: 0.5130 - val_accuracy: 0.7531\n",
            "Epoch 71/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7362 - val_loss: 0.5127 - val_accuracy: 0.7531\n",
            "Epoch 72/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7362 - val_loss: 0.5123 - val_accuracy: 0.7531\n",
            "Epoch 73/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7362 - val_loss: 0.5123 - val_accuracy: 0.7531\n",
            "Epoch 74/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7362 - val_loss: 0.5122 - val_accuracy: 0.7531\n",
            "Epoch 75/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7362 - val_loss: 0.5121 - val_accuracy: 0.7531\n",
            "Epoch 76/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7362 - val_loss: 0.5119 - val_accuracy: 0.7531\n",
            "Epoch 77/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7362 - val_loss: 0.5119 - val_accuracy: 0.7531\n",
            "Epoch 78/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7362 - val_loss: 0.5117 - val_accuracy: 0.7531\n",
            "Epoch 79/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7362 - val_loss: 0.5116 - val_accuracy: 0.7531\n",
            "Epoch 80/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7362 - val_loss: 0.5117 - val_accuracy: 0.7531\n",
            "Epoch 81/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7362 - val_loss: 0.5112 - val_accuracy: 0.7531\n",
            "Epoch 82/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7362 - val_loss: 0.5112 - val_accuracy: 0.7531\n",
            "Epoch 83/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7362 - val_loss: 0.5111 - val_accuracy: 0.7531\n",
            "Epoch 84/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7362 - val_loss: 0.5109 - val_accuracy: 0.7531\n",
            "Epoch 85/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7362 - val_loss: 0.5108 - val_accuracy: 0.7531\n",
            "Epoch 86/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5222 - accuracy: 0.7362 - val_loss: 0.5105 - val_accuracy: 0.7531\n",
            "Epoch 87/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.7362 - val_loss: 0.5105 - val_accuracy: 0.7531\n",
            "Epoch 88/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5220 - accuracy: 0.7362 - val_loss: 0.5104 - val_accuracy: 0.7531\n",
            "Epoch 89/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7362 - val_loss: 0.5106 - val_accuracy: 0.7531\n",
            "Epoch 90/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7362 - val_loss: 0.5105 - val_accuracy: 0.7531\n",
            "Epoch 91/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7362 - val_loss: 0.5103 - val_accuracy: 0.7531\n",
            "Epoch 92/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7362 - val_loss: 0.5099 - val_accuracy: 0.7531\n",
            "Epoch 93/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7362 - val_loss: 0.5102 - val_accuracy: 0.7531\n",
            "Epoch 94/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7362 - val_loss: 0.5101 - val_accuracy: 0.7531\n",
            "Epoch 95/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7362 - val_loss: 0.5100 - val_accuracy: 0.7531\n",
            "Epoch 96/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7362 - val_loss: 0.5097 - val_accuracy: 0.7531\n",
            "Epoch 97/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7362 - val_loss: 0.5100 - val_accuracy: 0.7531\n",
            "Epoch 98/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7362 - val_loss: 0.5097 - val_accuracy: 0.7531\n",
            "Epoch 99/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7362 - val_loss: 0.5096 - val_accuracy: 0.7531\n",
            "Epoch 100/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7362 - val_loss: 0.5097 - val_accuracy: 0.7531\n",
            "Epoch 101/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7362 - val_loss: 0.5094 - val_accuracy: 0.7531\n",
            "Epoch 102/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7362 - val_loss: 0.5096 - val_accuracy: 0.7531\n",
            "Epoch 103/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.7362 - val_loss: 0.5096 - val_accuracy: 0.7531\n",
            "Epoch 104/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7362 - val_loss: 0.5093 - val_accuracy: 0.7531\n",
            "Epoch 105/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7362 - val_loss: 0.5090 - val_accuracy: 0.7531\n",
            "Epoch 106/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7362 - val_loss: 0.5090 - val_accuracy: 0.7531\n",
            "Epoch 107/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7362 - val_loss: 0.5088 - val_accuracy: 0.7531\n",
            "Epoch 108/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7362 - val_loss: 0.5089 - val_accuracy: 0.7531\n",
            "Epoch 109/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7362 - val_loss: 0.5087 - val_accuracy: 0.7531\n",
            "Epoch 110/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7362 - val_loss: 0.5085 - val_accuracy: 0.7531\n",
            "Epoch 111/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7362 - val_loss: 0.5083 - val_accuracy: 0.7531\n",
            "Epoch 112/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7362 - val_loss: 0.5086 - val_accuracy: 0.7531\n",
            "Epoch 113/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7362 - val_loss: 0.5084 - val_accuracy: 0.7531\n",
            "Epoch 114/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7362 - val_loss: 0.5083 - val_accuracy: 0.7531\n",
            "Epoch 115/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7362 - val_loss: 0.5081 - val_accuracy: 0.7531\n",
            "Epoch 116/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7362 - val_loss: 0.5083 - val_accuracy: 0.7531\n",
            "Epoch 117/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7362 - val_loss: 0.5081 - val_accuracy: 0.7531\n",
            "Epoch 118/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7362 - val_loss: 0.5080 - val_accuracy: 0.7531\n",
            "Epoch 119/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.7362 - val_loss: 0.5081 - val_accuracy: 0.7531\n",
            "Epoch 120/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7362 - val_loss: 0.5077 - val_accuracy: 0.7531\n",
            "Epoch 121/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7362 - val_loss: 0.5076 - val_accuracy: 0.7531\n",
            "Epoch 122/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7362 - val_loss: 0.5074 - val_accuracy: 0.7531\n",
            "Epoch 123/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7362 - val_loss: 0.5073 - val_accuracy: 0.7531\n",
            "Epoch 124/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7362 - val_loss: 0.5073 - val_accuracy: 0.7531\n",
            "Epoch 125/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7362 - val_loss: 0.5073 - val_accuracy: 0.7531\n",
            "Epoch 126/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.7362 - val_loss: 0.5074 - val_accuracy: 0.7531\n",
            "Epoch 127/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7362 - val_loss: 0.5073 - val_accuracy: 0.7531\n",
            "Epoch 128/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7362 - val_loss: 0.5070 - val_accuracy: 0.7531\n",
            "Epoch 129/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.7362 - val_loss: 0.5071 - val_accuracy: 0.7531\n",
            "Epoch 130/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7362 - val_loss: 0.5070 - val_accuracy: 0.7531\n",
            "Epoch 131/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7362 - val_loss: 0.5071 - val_accuracy: 0.7531\n",
            "Epoch 132/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7362 - val_loss: 0.5068 - val_accuracy: 0.7531\n",
            "Epoch 133/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7362 - val_loss: 0.5067 - val_accuracy: 0.7531\n",
            "Epoch 134/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7362 - val_loss: 0.5067 - val_accuracy: 0.7531\n",
            "Epoch 135/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7362 - val_loss: 0.5067 - val_accuracy: 0.7531\n",
            "Epoch 136/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7362 - val_loss: 0.5067 - val_accuracy: 0.7531\n",
            "Epoch 137/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7362 - val_loss: 0.5066 - val_accuracy: 0.7531\n",
            "Epoch 138/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7362 - val_loss: 0.5067 - val_accuracy: 0.7531\n",
            "Epoch 139/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7362 - val_loss: 0.5065 - val_accuracy: 0.7531\n",
            "Epoch 140/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7362 - val_loss: 0.5063 - val_accuracy: 0.7531\n",
            "Epoch 141/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.7423 - val_loss: 0.5064 - val_accuracy: 0.7654\n",
            "Epoch 142/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7393 - val_loss: 0.5065 - val_accuracy: 0.7654\n",
            "Epoch 143/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7331 - val_loss: 0.5061 - val_accuracy: 0.7531\n",
            "Epoch 144/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7331 - val_loss: 0.5064 - val_accuracy: 0.7654\n",
            "Epoch 145/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7546 - val_loss: 0.5064 - val_accuracy: 0.7654\n",
            "Epoch 146/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7546 - val_loss: 0.5064 - val_accuracy: 0.7654\n",
            "Epoch 147/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7546 - val_loss: 0.5065 - val_accuracy: 0.7654\n",
            "Epoch 148/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7546 - val_loss: 0.5062 - val_accuracy: 0.7654\n",
            "Epoch 149/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7546 - val_loss: 0.5061 - val_accuracy: 0.7654\n",
            "Epoch 150/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7546 - val_loss: 0.5062 - val_accuracy: 0.7654\n",
            "Epoch 151/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7546 - val_loss: 0.5060 - val_accuracy: 0.7654\n",
            "Epoch 152/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7546 - val_loss: 0.5059 - val_accuracy: 0.7654\n",
            "Epoch 153/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7546 - val_loss: 0.5058 - val_accuracy: 0.7654\n",
            "Epoch 154/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7546 - val_loss: 0.5057 - val_accuracy: 0.7654\n",
            "Epoch 155/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7546 - val_loss: 0.5059 - val_accuracy: 0.7654\n",
            "Epoch 156/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7546 - val_loss: 0.5059 - val_accuracy: 0.7654\n",
            "Epoch 157/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7546 - val_loss: 0.5058 - val_accuracy: 0.7654\n",
            "Epoch 158/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7546 - val_loss: 0.5056 - val_accuracy: 0.7654\n",
            "Epoch 159/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7546 - val_loss: 0.5055 - val_accuracy: 0.7654\n",
            "Epoch 160/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7546 - val_loss: 0.5057 - val_accuracy: 0.7654\n",
            "Epoch 161/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7546 - val_loss: 0.5054 - val_accuracy: 0.7654\n",
            "Epoch 162/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7546 - val_loss: 0.5054 - val_accuracy: 0.7654\n",
            "Epoch 163/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7546 - val_loss: 0.5054 - val_accuracy: 0.7654\n",
            "Epoch 164/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7546 - val_loss: 0.5054 - val_accuracy: 0.7654\n",
            "Epoch 165/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7546 - val_loss: 0.5053 - val_accuracy: 0.7654\n",
            "Epoch 166/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7546 - val_loss: 0.5054 - val_accuracy: 0.7654\n",
            "Epoch 167/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7546 - val_loss: 0.5055 - val_accuracy: 0.7654\n",
            "Epoch 168/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7546 - val_loss: 0.5051 - val_accuracy: 0.7654\n",
            "Epoch 169/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7546 - val_loss: 0.5051 - val_accuracy: 0.7654\n",
            "Epoch 170/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7546 - val_loss: 0.5051 - val_accuracy: 0.7654\n",
            "Epoch 171/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7546 - val_loss: 0.5051 - val_accuracy: 0.7654\n",
            "Epoch 172/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7546 - val_loss: 0.5053 - val_accuracy: 0.7654\n",
            "Epoch 173/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5050 - val_accuracy: 0.7654\n",
            "Epoch 174/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5051 - val_accuracy: 0.7654\n",
            "Epoch 175/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7546 - val_loss: 0.5050 - val_accuracy: 0.7654\n",
            "Epoch 176/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7546 - val_loss: 0.5049 - val_accuracy: 0.7654\n",
            "Epoch 177/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7546 - val_loss: 0.5046 - val_accuracy: 0.7654\n",
            "Epoch 178/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5050 - val_accuracy: 0.7654\n",
            "Epoch 179/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7546 - val_loss: 0.5049 - val_accuracy: 0.7654\n",
            "Epoch 180/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7546 - val_loss: 0.5044 - val_accuracy: 0.7654\n",
            "Epoch 181/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7546 - val_loss: 0.5048 - val_accuracy: 0.7654\n",
            "Epoch 182/250\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.7546 - val_loss: 0.5045 - val_accuracy: 0.7654\n",
            "Epoch 183/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5044 - val_accuracy: 0.7654\n",
            "Epoch 184/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5044 - val_accuracy: 0.7654\n",
            "Epoch 185/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7546 - val_loss: 0.5044 - val_accuracy: 0.7654\n",
            "Epoch 186/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5046 - val_accuracy: 0.7654\n",
            "Epoch 187/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5045 - val_accuracy: 0.7654\n",
            "Epoch 188/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5043 - val_accuracy: 0.7654\n",
            "Epoch 189/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7546 - val_loss: 0.5044 - val_accuracy: 0.7654\n",
            "Epoch 190/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7546 - val_loss: 0.5044 - val_accuracy: 0.7654\n",
            "Epoch 191/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7546 - val_loss: 0.5042 - val_accuracy: 0.7654\n",
            "Epoch 192/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7546 - val_loss: 0.5041 - val_accuracy: 0.7654\n",
            "Epoch 193/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7546 - val_loss: 0.5039 - val_accuracy: 0.7654\n",
            "Epoch 194/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5040 - val_accuracy: 0.7654\n",
            "Epoch 195/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7546 - val_loss: 0.5040 - val_accuracy: 0.7654\n",
            "Epoch 196/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7546 - val_loss: 0.5037 - val_accuracy: 0.7654\n",
            "Epoch 197/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5038 - val_accuracy: 0.7654\n",
            "Epoch 198/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5038 - val_accuracy: 0.7654\n",
            "Epoch 199/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5037 - val_accuracy: 0.7654\n",
            "Epoch 200/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5038 - val_accuracy: 0.7654\n",
            "Epoch 201/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5039 - val_accuracy: 0.7654\n",
            "Epoch 202/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5037 - val_accuracy: 0.7654\n",
            "Epoch 203/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7546 - val_loss: 0.5033 - val_accuracy: 0.7654\n",
            "Epoch 204/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7546 - val_loss: 0.5038 - val_accuracy: 0.7654\n",
            "Epoch 205/250\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5036 - val_accuracy: 0.7654\n",
            "Epoch 206/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7546 - val_loss: 0.5039 - val_accuracy: 0.7654\n",
            "Epoch 207/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5035 - val_accuracy: 0.7654\n",
            "Epoch 208/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5033 - val_accuracy: 0.7654\n",
            "Epoch 209/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5038 - val_accuracy: 0.7654\n",
            "Epoch 210/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7546 - val_loss: 0.5037 - val_accuracy: 0.7654\n",
            "Epoch 211/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5035 - val_accuracy: 0.7654\n",
            "Epoch 212/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7546 - val_loss: 0.5035 - val_accuracy: 0.7654\n",
            "Epoch 213/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7546 - val_loss: 0.5037 - val_accuracy: 0.7654\n",
            "Epoch 214/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7546 - val_loss: 0.5036 - val_accuracy: 0.7654\n",
            "Epoch 215/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5032 - val_accuracy: 0.7654\n",
            "Epoch 216/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7546 - val_loss: 0.5034 - val_accuracy: 0.7654\n",
            "Epoch 217/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5033 - val_accuracy: 0.7654\n",
            "Epoch 218/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7546 - val_loss: 0.5036 - val_accuracy: 0.7654\n",
            "Epoch 219/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7546 - val_loss: 0.5035 - val_accuracy: 0.7654\n",
            "Epoch 220/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7546 - val_loss: 0.5035 - val_accuracy: 0.7654\n",
            "Epoch 221/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7546 - val_loss: 0.5033 - val_accuracy: 0.7654\n",
            "Epoch 222/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7546 - val_loss: 0.5032 - val_accuracy: 0.7654\n",
            "Epoch 223/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5035 - val_accuracy: 0.7654\n",
            "Epoch 224/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7546 - val_loss: 0.5030 - val_accuracy: 0.7654\n",
            "Epoch 225/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.7546 - val_loss: 0.5034 - val_accuracy: 0.7654\n",
            "Epoch 226/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7546 - val_loss: 0.5031 - val_accuracy: 0.7654\n",
            "Epoch 227/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7546 - val_loss: 0.5034 - val_accuracy: 0.7654\n",
            "Epoch 228/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7546 - val_loss: 0.5031 - val_accuracy: 0.7654\n",
            "Epoch 229/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7546 - val_loss: 0.5033 - val_accuracy: 0.7654\n",
            "Epoch 230/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.7546 - val_loss: 0.5031 - val_accuracy: 0.7654\n",
            "Epoch 231/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7546 - val_loss: 0.5029 - val_accuracy: 0.7654\n",
            "Epoch 232/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7546 - val_loss: 0.5033 - val_accuracy: 0.7654\n",
            "Epoch 233/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7546 - val_loss: 0.5026 - val_accuracy: 0.7654\n",
            "Epoch 234/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7546 - val_loss: 0.5029 - val_accuracy: 0.7654\n",
            "Epoch 235/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7546 - val_loss: 0.5029 - val_accuracy: 0.7654\n",
            "Epoch 236/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7546 - val_loss: 0.5028 - val_accuracy: 0.7654\n",
            "Epoch 237/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7546 - val_loss: 0.5028 - val_accuracy: 0.7654\n",
            "Epoch 238/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7546 - val_loss: 0.5028 - val_accuracy: 0.7654\n",
            "Epoch 239/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.7546 - val_loss: 0.5031 - val_accuracy: 0.7654\n",
            "Epoch 240/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7546 - val_loss: 0.5029 - val_accuracy: 0.7654\n",
            "Epoch 241/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7546 - val_loss: 0.5027 - val_accuracy: 0.7654\n",
            "Epoch 242/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7546 - val_loss: 0.5027 - val_accuracy: 0.7654\n",
            "Epoch 243/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7546 - val_loss: 0.5027 - val_accuracy: 0.7654\n",
            "Epoch 244/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7546 - val_loss: 0.5028 - val_accuracy: 0.7654\n",
            "Epoch 245/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7546 - val_loss: 0.5029 - val_accuracy: 0.7654\n",
            "Epoch 246/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7546 - val_loss: 0.5026 - val_accuracy: 0.7654\n",
            "Epoch 247/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7546 - val_loss: 0.5025 - val_accuracy: 0.7654\n",
            "Epoch 248/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7546 - val_loss: 0.5032 - val_accuracy: 0.7654\n",
            "Epoch 249/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7546 - val_loss: 0.5027 - val_accuracy: 0.7654\n",
            "Epoch 250/250\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7546 - val_loss: 0.5026 - val_accuracy: 0.7654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt6tO4B3-zYW",
        "outputId": "3e7c1897-6233-452b-c6a4-c630babc8fff"
      },
      "source": [
        "print(history.params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'verbose': 1, 'epochs': 250, 'steps': 33}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZM1AJJe6w6mT",
        "outputId": "eb68419c-14e7-4a7d-d7b4-82ec92ce760c"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data' , 'validation data'],loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZXl9oUw6QHp"
      },
      "source": [
        "model.load_weights('MyBestModel.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXOjVcZ46gKP",
        "outputId": "3a2adfc7-a296-457f-f0e6-8c0f00936a02"
      },
      "source": [
        "#Evaluating the model on training set\n",
        "scores=model.evaluate(XTrain,YTrain)\n",
        "print(model.metrics_names)\n",
        "print(scores)\n",
        "print(model.metrics_names[1], scores[1]*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 0s 1ms/step - loss: 0.5166 - accuracy: 0.7546\n",
            "['loss', 'accuracy']\n",
            "[0.5166025161743164, 0.754601240158081]\n",
            "accuracy 75.4601240158081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VuAQJ5J8cYX",
        "outputId": "d1ca9046-b393-4fdd-cdd7-82674ac697a1"
      },
      "source": [
        "#Evaluating the model on valisation set\n",
        "scores=model.evaluate(XValidation,YValidation)\n",
        "print(model.metrics_names)\n",
        "print(scores)\n",
        "print(model.metrics_names[1], scores[1]*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7654\n",
            "['loss', 'accuracy']\n",
            "[0.5025140643119812, 0.7654321193695068]\n",
            "accuracy 76.54321193695068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVvHL5C6_k_O",
        "outputId": "b4d95436-668e-4eff-efe3-5a41865f8922"
      },
      "source": [
        "#Checking the predictions\n",
        "print(XValidation[0:20])\n",
        "print(YValidation[0:20])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.8 -0.5 0.6 -0.2]\n",
            " [0.8 -0.6 0.6 -0.2]\n",
            " [0.8 -0.6 0.6 -0.2]\n",
            " [-1.3 -0.2 -1.7 -0.2]\n",
            " [0.8 0.0 -1.7 -0.2]\n",
            " [-1.3 -0.6 0.6 -0.2]\n",
            " [0.8 -0.6 0.6 -0.2]\n",
            " [-1.3 0.4 0.6 -0.2]\n",
            " [0.8 -0.6 0.6 -0.2]\n",
            " [-1.3 -0.3 0.6 -0.2]\n",
            " [-1.3 2.9 0.6 -0.2]\n",
            " [-1.3 -0.4 0.6 -0.2]\n",
            " [0.8 -0.6 -1.7 -0.2]\n",
            " [-1.3 -0.3 0.6 -0.2]\n",
            " [-1.3 -0.4 0.6 -0.2]\n",
            " [-1.3 2.7 0.6 -0.0]\n",
            " [0.8 0.6 0.6 -0.2]\n",
            " [-1.3 -0.6 -1.7 -0.2]\n",
            " [0.8 -0.5 0.6 0.1]\n",
            " [-1.3 -0.6 0.6 -0.2]]\n",
            "[1.0 1.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 1.0 0.0\n",
            " 1.0 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bejGNrl1_1iP",
        "outputId": "549b7e7a-b7dd-4ed7-b1f9-c701db31b5aa"
      },
      "source": [
        "prediction=model.predict(XValidation)\n",
        "print(prediction[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.8]\n",
            " [0.8]\n",
            " [0.8]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.8]\n",
            " [0.8]\n",
            " [0.8]\n",
            " [0.8]\n",
            " [0.8]\n",
            " [0.9]\n",
            " [0.8]\n",
            " [0.5]\n",
            " [0.8]\n",
            " [0.8]\n",
            " [0.9]\n",
            " [0.9]\n",
            " [0.5]\n",
            " [0.8]\n",
            " [0.8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSZJcSAfKr1_",
        "outputId": "c8bbafb2-67d4-4e0d-a9de-87b176d126ba"
      },
      "source": [
        "print(prediction[0:20].round())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [0.0]\n",
            " [0.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [0.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [1.0]\n",
            " [0.0]\n",
            " [1.0]\n",
            " [1.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yChCRfQUDUz5",
        "outputId": "a6137c35-b2b0-41ef-ec9a-575f93184e32"
      },
      "source": [
        "#Test with some data\n",
        "XTest=[[0,20,0,0],\n",
        "       [1,55,1,100],\n",
        "       [1,40,1,30]]\n",
        "XTest -=mean\n",
        "XTest /=std\n",
        "pred=model.predict(XTest)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5]\n",
            " [0.9]\n",
            " [0.9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "vSPtKDnUJ9mB",
        "outputId": "2c1b77cb-7c5d-48ce-a657-0eac54f5fdc4"
      },
      "source": [
        "plt.plot(YValidation , prediction , '.',alpha=0.3)\n",
        "plt.xlabel('correct labels')\n",
        "plt.ylabel('predicted scores')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d2k6BC4SIuz"
      },
      "source": [
        "model.save('MyBestModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba3YDnUmSWNL",
        "outputId": "bfcf781b-40ca-4dcf-8f61-9441fd71540a"
      },
      "source": [
        "from tensorflow.python import keras\n",
        "import tensorflow\n",
        "MyModel=keras.models.load_model('/content/MyBestModel.h5')\n",
        "converter=tensorflow.lite.TFLiteConverter.from_keras_model(MyModel)\n",
        "TfLiteModel=converter.convert()\n",
        "open(\"model.tflite\",\"wb\").write(TfLiteModel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpt4_qrhmy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1912"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KSc7WYOUt1S",
        "outputId": "71451497-3faf-4af8-ebea-40ac310caf66"
      },
      "source": [
        "MyModel.predict(XTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5],\n",
              "       [0.9],\n",
              "       [0.9]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zATMj0tD8KwJ",
        "outputId": "abca9bf3-6c50-48b3-bf30-e777940a9bb0"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/19/5ab51223cb178ab0f44fbc2ac494d015da135c70603a13eecb2328a59000/tensorflowjs-3.7.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 24.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 30kB 27.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40kB 30.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 51kB 25.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 61kB 27.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.12.4)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.34.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (1.30.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (4.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Installing collected packages: tensorflowjs\n",
            "Successfully installed tensorflowjs-3.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF07UKE68SfL"
      },
      "source": [
        "import tensorflowjs as tfjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHI1Pzlk8cv5"
      },
      "source": [
        " tfjs.converters.save_keras_model(model, \"/contents/\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG2yrwpR82zl",
        "outputId": "735c20d4-6dfb-44d2-ea6c-1a259ca278aa"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.tflite  MyBestModel.h5  MyBestModel.hdf5\tMyDataset1.csv\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVYq2wzv9c8Q"
      },
      "source": [
        "tfjs.converters.save_keras_model(model, \"/contents/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}